{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LSTUR: Neural News Recommendation with Long- and Short-term User Representations\n",
                "LSTUR \\[1\\] is a news recommendation approach capturing users' both long-term preferences and short-term interests. We will use this algorithm to perform the necessary ranking of the Ebnerd-dataset.\n",
                "\n",
                "## Data format:\n",
                "The dataformat and available data can be found [here](https://recsys.eb.dk/dataset/), you can select between demo, small and large with an extra testset available. We transformed the data by manipulating, reordering and dropping columns into the following format. This should be suitable for the algorithms implementation.\n",
                " \n",
                "### article data\n",
                "This file contains news information including articleId, category, title, body and url.\n",
                "One simple example: <br>\n",
                "\n",
                "`3044020\tunderholdning\tPrins Harry tvunget til dna-test\tDen britiske tabloidavis The Sun fortsætter med at lække historier fra den kommende bog om prinsesse Diana, skrevet af prinsessens veninde Simone Simmons.Onsdag er det historien om, at det britiske kongehus lod prins Harry dna-teste for at sikre sig, at prins Charles var far til ham.Hoffet frygtede, at Dianas tidligere elsker James Hewitt, var far til Harry.Dna-testen fandt sted, da Harry var 11 år gammel.Det var en slet skjult hemmelighed, at Diana og Hewitt hyggede sig i sengen, og der var simpelthen en udbredt frygt på Buckingham Palace for, at lidenskaben havde resulteret i rødhårede Harry.Diana selv afviste rygterne og påpegede, at hvis man regnede på datoerne, kunne Hewitt ikke være far til Harry, men frygten for arvefølgen var så stor, at den 11-årige Harry måtte tage testen trods Dianas forsikringer om hans fædrene ophav.\thttps://ekstrabladet.dk/underholdning/udlandkendte/article3044020.ece\n",
                "`\n",
                "<br>\n",
                "\n",
                "In general, each line in data file represents information of one piece of news: <br>\n",
                "\n",
                "`[Article ID] [Category] [Article Title] [Articles Body] [Articles Url]`\n",
                "\n",
                "<br>\n",
                "\n",
                "We generate a word_dict file to tranform words in news title to word indexes, and a embedding matrix is initted from pretrained glove embeddings.\n",
                "\n",
                "### behaviors data\n",
                "One simple example: <br>\n",
                "`1\tU82271\t11/11/2019 3:28:58 PM\tN3130 N11621 N12917 N4574 N12140 N9748\tN13390-0 N7180-0 N20785-0 N6937-0 N15776-0 N25810-0 N20820-0 N6885-0 N27294-0 N18835-0 N16945-0 N7410-0 N23967-0 N22679-0 N20532-0 N26651-0 N22078-0 N4098-0 N16473-0 N13841-0 N15660-0 N25787-0 N2315-0 N1615-0 N9087-0 N23880-0 N3600-0 N24479-0 N22882-0 N26308-0 N13594-0 N2220-0 N28356-0 N17083-0 N21415-0 N18671-0 N9440-0 N17759-0 N10861-0 N21830-0 N8064-0 N5675-0 N15037-0 N26154-0 N15368-1 N481-0 N3256-0 N20663-0 N23940-0 N7654-0 N10729-0 N7090-0 N23596-0 N15901-0 N16348-0 N13645-0 N8124-0 N20094-0 N27774-0 N23011-0 N14832-0 N15971-0 N27729-0 N2167-0 N11186-0 N18390-0 N21328-0 N10992-0 N20122-0 N1958-0 N2004-0 N26156-0 N17632-0 N26146-0 N17322-0 N18403-0 N17397-0 N18215-0 N14475-0 N9781-0 N17958-0 N3370-0 N1127-0 N15525-0 N12657-0 N10537-0 N18224-0`\n",
                "<br>\n",
                "\n",
                "In general, each line in data file represents one instance of an impression. The format is like: <br>\n",
                "\n",
                "`[Impression ID] [User ID] [Impression Time] [User Click History] [Impression News]`\n",
                "\n",
                "<br>\n",
                "\n",
                "User Click History is the user historical clicked news before Impression Time. Impression News is the displayed news in an impression, which format is:<br>\n",
                "\n",
                "`[News ID 1]-[label1] ... [News ID n]-[labeln]`\n",
                "\n",
                "<br>\n",
                "Label represents whether the news is clicked by the user. All information of news in User Click History and Impression News can be found in news data file."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Imports and Global settings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "System version: 3.11.8 (main, Feb  6 2024, 21:21:21) [Clang 15.0.0 (clang-1500.1.0.2.5)]\n",
                        "Tensorflow version: 2.15.1\n"
                    ]
                }
            ],
            "source": [
                "import os, sys, zipfile, logging\n",
                "import numpy as np\n",
                "import polars as pl\n",
                "import tensorflow as tf\n",
                "\n",
                "from pathlib import Path\n",
                "from tqdm import tqdm\n",
                "\n",
                "from recommenders.models.deeprec.deeprec_utils import download_deeprec_resources \n",
                "from recommenders.models.newsrec.newsrec_utils import prepare_hparams\n",
                "from recommenders.models.newsrec.models.lstur import LSTURModel\n",
                "from recommenders.models.newsrec.newsrec_utils import get_mind_data_set\n",
                "from recommenders.utils.notebook_utils import store_metadata\n",
                "\n",
                "print(\"System version: {}\".format(sys.version))\n",
                "print(\"Tensorflow version: {}\".format(tf.__version__))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Configure logging settings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "polars.config.Config"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# configurations\n",
                "tf.get_logger().setLevel('ERROR') # only show error messages\n",
                "\n",
                "logging.basicConfig(level=logging.INFO)\n",
                "LOG = logging.getLogger(__name__)\n",
                "LOG.setLevel(logging.INFO)\n",
                "\n",
                "pl.Config.set_tbl_rows(100)\n",
                "pl.Config.set_streaming_chunk_size(500_000)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Prepare Parameters and File Locations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "tags": [
                    "parameters"
                ]
            },
            "outputs": [],
            "source": [
                "# LSTUR parameters\n",
                "EPOCHS = 5\n",
                "SEED = 40\n",
                "BATCH_SIZE = 32\n",
                "\n",
                "# whether to re-compute the dataset\n",
                "FORCE_RELOAD = True\n",
                "\n",
                "# path to the dataset\n",
                "DATASET_NAME = \"demo\" # one of: demo, small, large\n",
                "TEMP_DIR = \"tmp\"\n",
                "GROUP_PROJECT_PATH = \"/Users/maxkleinegger/Downloads/\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:__main__:/Users/maxkleinegger/Downloads/demo\n"
                    ]
                }
            ],
            "source": [
                "# dataset parquet files path\n",
                "PATH = Path(os.path.join(GROUP_PROJECT_PATH, DATASET_NAME))\n",
                "train_behaviors_path = os.path.join(PATH, 'train', 'behaviors.parquet')\n",
                "train_history_path = os.path.join(PATH, 'train', 'history.parquet')\n",
                "val_behaviors_path = os.path.join(PATH, 'validation', 'behaviors.parquet')\n",
                "val_history_path = os.path.join(PATH, 'validation', 'history.parquet')\n",
                "articles_path = os.path.join(PATH, 'articles.parquet')\n",
                "\n",
                "LOG.info(PATH)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "# artifacts file path\n",
                "TMP_PATH = Path(os.path.join(GROUP_PROJECT_PATH, TEMP_DIR))\n",
                "tmp_train_path = Path(os.path.join(TMP_PATH, DATASET_NAME, 'train'))\n",
                "tmp_val_path = Path(os.path.join(TMP_PATH, DATASET_NAME, 'val'))\n",
                "\n",
                "# create directories if not exist\n",
                "tmp_train_path.mkdir(exist_ok=True, parents=True)\n",
                "tmp_val_path.mkdir(exist_ok=True, parents=True)\n",
                "\n",
                "train_behaviors_file = os.path.join(tmp_train_path, 'behaviors.tsv')\n",
                "val_behaviors_file = os.path.join(tmp_val_path, 'behaviors.tsv')\n",
                "articles_file = os.path.join(TMP_PATH, 'articles.tsv')\n",
                "\n",
                "# hyperparameters\n",
                "yaml_file = os.path.join('../src/group_33/configs/lstur.yaml')\n",
                "user_dict_file = os.path.join(tmp_train_path, 'user_dict.pkl')\n",
                "words_dict_file = os.path.join(tmp_train_path, 'words_dict.pkl')\n",
                "word_embeddings_file = os.path.join(tmp_train_path, 'word_embeddings.npy')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Transform parquet files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "ename": "SyntaxError",
                    "evalue": "f-string: unmatched '[' (1512826099.py, line 55)",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 55\u001b[0;36m\u001b[0m\n\u001b[0;31m    user_history[f\"U{row[\"user_id\"]}\"] = {\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string: unmatched '['\n"
                    ]
                }
            ],
            "source": [
                "COL_IMPRESSION_ID_IDX = 0\n",
                "COL_USER_ID_IDX = 8\n",
                "COL_USER_ID = \"user_id\"\n",
                "COL_IMPRESSION_TIME_IDX = 2\n",
                "COL_INVIEW_ARTICLE_IDS_IDX = 6\n",
                "COL_CLICKED_ARTICLE_IDS_IDX = 7\n",
                "\n",
                "\n",
                "def transfrom_behavior_file(\n",
                "    behaviors_path, history_path, result_path, history_size=None, sample_size=1\n",
                "):\n",
                "    def transform_row(row):\n",
                "        impression_id = row[COL_IMPRESSION_ID_IDX]\n",
                "        user_id = row[COL_USER_ID_IDX]\n",
                "        impression_time = row[COL_IMPRESSION_TIME_IDX]\n",
                "        clicked_articles = user_history.get(user_id, {}).get(\"article_id\", [])\n",
                "        timestamps = user_history.get(user_id, {}).get(\"impression_time\", [])\n",
                "\n",
                "        # Filter click history to include only clicks before the impression time\n",
                "        user_click_history = [\n",
                "            article_id\n",
                "            for article_id, timestamp in zip(clicked_articles, timestamps)\n",
                "            if timestamp < impression_time\n",
                "        ]\n",
                "\n",
                "        user_click_history = user_click_history[-history_size:]\n",
                "        user_click_history_str = \" \".join(user_click_history)\n",
                "\n",
                "        # Prepare impression news\n",
                "        inview_articles = row[COL_INVIEW_ARTICLE_IDS_IDX]\n",
                "        clicked_articles = row[COL_CLICKED_ARTICLE_IDS_IDX]\n",
                "        impression_news = [\n",
                "            f\"{article_id}-{1 if article_id in clicked_articles else 0}\"\n",
                "            for article_id in inview_articles\n",
                "        ]\n",
                "        impression_news_str = \" \".join(impression_news)\n",
                "\n",
                "        return (\n",
                "            impression_id,\n",
                "            user_id,\n",
                "            impression_time,\n",
                "            user_click_history_str,\n",
                "            impression_news_str,\n",
                "        )\n",
                "\n",
                "    behaviors = pl.read_parquet(behaviors_path)\n",
                "    history = pl.read_parquet(history_path)\n",
                "\n",
                "    if history_size is None:\n",
                "        history_size = history.shape[0]\n",
                "\n",
                "    # Transform history to a dictionary for fast lookup\n",
                "    user_history = {}\n",
                "    for row in history.iter_rows(named=True):\n",
                "        user_history[f\"U{row['user_id']}\"] = {\n",
                "            \"article_id\": row[\"article_id_fixed\"],\n",
                "            \"impression_time\": row[\"impression_time_fixed\"],\n",
                "        }\n",
                "\n",
                "    result_behavior_df = pl.DataFrame(behaviors.map_rows(transform_row))\n",
                "    result_behavior_df.columns = [\n",
                "        \"Impression ID\",\n",
                "        \"User ID\",\n",
                "        \"Impression Time\",\n",
                "        \"User Click History\",\n",
                "        \"Impression News\",\n",
                "    ]\n",
                "    result_behavior_df.sample(fraction=sample_size).write_csv(\n",
                "        result_path, quote_style=\"never\", include_header=False, separator=\"\\t\"\n",
                "    )\n",
                "    return result_behavior_df\n",
                "\n",
                "\n",
                "\n",
                "if not Path(train_behaviors_file).exists() or FORCE_RELOAD:\n",
                "    behavior_df = transfrom_behavior_file(train_behaviors_path, train_history_path, train_behaviors_file)\n",
                "if not Path(val_behaviors_file).exists() or FORCE_RELOAD:\n",
                "    behavior_df_val = transfrom_behavior_file(val_behaviors_path, val_history_path, val_behaviors_file)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "COL_ARTICLE_ID = \"article_id\"\n",
                "COL_ARTICLE_CATEGORY = \"category_str\"\n",
                "COL_ARTICLE_TITLE = \"title\"\n",
                "COL_ARTICLE_BODY = \"body\"\n",
                "COL_ARTICLE_URL = \"url\"\n",
                "\n",
                "def transform_articles_file(articles_path, result_path):\n",
                "    def clean_text(column):\n",
                "        return column.str.replace_all(\"\\n\", \"\").str.replace_all(\"\\t\", \" \")\n",
                "\n",
                "    articles = pl.read_parquet(articles_path)\n",
                "\n",
                "    # Select relevant columns and apply the cleaning function to 'title' and 'body'\n",
                "    articles = articles.select(\n",
                "        [\n",
                "            COL_ARTICLE_ID,\n",
                "            COL_ARTICLE_CATEGORY,\n",
                "            COL_ARTICLE_TITLE,\n",
                "            COL_ARTICLE_BODY,\n",
                "            COL_ARTICLE_URL,\n",
                "        ]\n",
                "    ).with_columns(\n",
                "        [\n",
                "            clean_text(articles[COL_ARTICLE_TITLE]),\n",
                "            clean_text(articles[COL_ARTICLE_BODY]),\n",
                "        ]\n",
                "    )\n",
                "\n",
                "    articles.write_csv(\n",
                "        result_path, quote_style=\"never\", include_header=False, separator=\"\\t\"\n",
                "    )\n",
                "    return articles\n",
                "\n",
                "\n",
                "\n",
                "if not Path(articles_file).exists() or FORCE_RELOAD:\n",
                "    train_news = transform_articles_file(articles_path, articles_file)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Create hyper-parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'behavior_df' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      2\u001b[0m user_id_mapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m         user_id: i\n\u001b[0;32m----> 4\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, user_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mbehavior_df\u001b[49m[COL_USER_ID]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m      5\u001b[0m     }\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Dump the dictionary as a pkl file\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(user_dict_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
                        "\u001b[0;31mNameError\u001b[0m: name 'behavior_df' is not defined"
                    ]
                }
            ],
            "source": [
                "import pickle\n",
                "user_id_mapping = {\n",
                "        user_id: i\n",
                "        for i, user_id in enumerate(behavior_df[COL_USER_ID].unique())\n",
                "    }\n",
                "\n",
                "# Dump the dictionary as a pkl file\n",
                "with open(user_dict_file, \"wb\") as f:\n",
                "    pickle.dump(user_id_mapping, f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 175,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pickle\n",
                "words = train_news['title'].str.split(' ').explode()\n",
                "words_id_mapping = {word: i for i, word in enumerate(words.unique())}\n",
                "\n",
                "# Dump the dictionary as a pkl file\n",
                "with open(words_dict_file, 'wb') as f:\n",
                "    pickle.dump(words_id_mapping, f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 279,
            "metadata": {},
            "outputs": [],
            "source": [
                "from recommenders.models.newsrec.newsrec_utils import word_tokenize\n",
                "\n",
                "# Tokenize the words\n",
                "tokens = train_news['title'].map_elements(word_tokenize)\n",
                "word2id = {word: i for i, word in enumerate(tokens.explode().unique())}\n",
                "# Dump the dictionary as a pkl file\n",
                "with open(words_dict_file, 'wb') as f:\n",
                "    pickle.dump(word2id, f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 210,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/maxkleinegger/Documents/TUWien/2024SS/RecSys/ue.nosync/Group_33/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
                        "  warnings.warn(\n",
                        "Encoding: 100%|██████████| 2593/2593 [03:32<00:00, 12.22it/s]\n"
                    ]
                }
            ],
            "source": [
                "from transformers import AutoTokenizer, AutoModel\n",
                "from ebrec.utils._nlp import generate_embeddings_with_transformers\n",
                "\n",
                "TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
                "MAX_TITLE_LENGTH = 30\n",
                "batch_size = 8\n",
                "text_list = train_news['title'].to_list()\n",
                "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
                "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
                "t = generate_embeddings_with_transformers(\n",
                "    transformer_model, transformer_tokenizer, text_list, batch_size, \"cpu\"\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 217,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.save(word_embeddings_file, t)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 285,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "# Function to load GloVe embeddings\n",
                "def load_glove_embeddings(glove_file_path):\n",
                "    embeddings_index = {}\n",
                "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
                "        for line in f:\n",
                "            values = line.split()\n",
                "            word = values[0]\n",
                "            coefs = np.asarray(values[1:], dtype='float32')\n",
                "            embeddings_index[word] = coefs\n",
                "    return embeddings_index\n",
                "\n",
                "# Load GloVe embeddings\n",
                "glove_file_path = os.path.abspath('/Users/maxkleinegger/Downloads/glove.6B/glove.6B.100d.txt')\n",
                "embeddings_index = load_glove_embeddings(glove_file_path)\n",
                "\n",
                "# Create embedding matrix\n",
                "embedding_dim = 100\n",
                "embedding_matrix = np.zeros((len(word2id), embedding_dim))\n",
                "\n",
                "for word, idx in word2id.items():\n",
                "    embedding_vector = embeddings_index.get(word)\n",
                "    if embedding_vector is not None:\n",
                "        embedding_matrix[idx] = embedding_vector\n",
                "    else:\n",
                "        # Words not found in the embedding index will be initialized randomly\n",
                "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
                "\n",
                "# Save the embedding matrix to a file\n",
                "np.save(word_embeddings_file, embedding_matrix)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 284,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array([[ 0.17757   , -0.29135001, -0.70521998, ...,  0.41793999,\n",
                            "        -0.30325001,  0.26774999],\n",
                            "       [-0.36452862, -0.07568185, -0.41076382, ..., -0.37178079,\n",
                            "         0.5245951 ,  1.03171141],\n",
                            "       [ 0.02057405, -0.50906059,  0.16129882, ..., -0.32023004,\n",
                            "         0.00815788, -0.86302826],\n",
                            "       ...,\n",
                            "       [-0.1811851 , -0.45838818,  0.47729788, ..., -0.35097379,\n",
                            "         0.11257933, -0.54628964],\n",
                            "       [-0.79680997, -0.27430999,  0.46555001, ..., -1.16209996,\n",
                            "        -0.035306  , -0.070289  ],\n",
                            "       [-0.23470999, -0.64236999, -0.19595   , ...,  0.43999001,\n",
                            "        -0.36699   ,  0.29971001]])"
                        ]
                    },
                    "execution_count": 284,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "embedding_matrix"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 176,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/maxkleinegger/Documents/TUWien/2024SS/RecSys/ue.nosync/Group_33/.venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
                        "  warnings.warn(\n"
                    ]
                }
            ],
            "source": [
                "from transformers import AutoTokenizer, AutoModel\n",
                "import numpy as np\n",
                "import torch\n",
                "\n",
                "from ebrec.utils._articles import convert_text2encoding_with_transformers\n",
                "from ebrec.utils._polars import concat_str_columns, slice_join_dataframes\n",
                "from ebrec.utils._articles import create_article_id_to_value_mapping\n",
                "from ebrec.utils._nlp import get_transformers_word_embeddings\n",
                "from ebrec.utils._python import write_submission_file, rank_predictions_by_score\n",
                "\n",
                "\n",
                "\n",
                "# LOAD HUGGINGFACE:\n",
                "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
                "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
                "\n",
                "# We'll init the word embeddings using the\n",
                "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
                "\n",
                "tokenized_titles = train_news['title'].map_elements(lambda x: transformer_tokenizer.tokenize(x))\n",
                "\n",
                "# Create dictionary mapping each word to an ID\n",
                "all_tokens = [token for title in tokenized_titles for token in title]\n",
                "unique_tokens = list(set(all_tokens))\n",
                "word2id = {word: idx for idx, word in enumerate(unique_tokens)}\n",
                "\n",
                "embeddings = {}\n",
                "for word, idx in word2id.items():\n",
                "    input_ids = torch.tensor([transformer_tokenizer.convert_tokens_to_ids(word)]).unsqueeze(0)  # Batch size 1\n",
                "    with torch.no_grad():\n",
                "        outputs = transformer_model(input_ids)\n",
                "    embeddings[word] = outputs.last_hidden_state.squeeze().mean(dim=0).numpy()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 182,
            "metadata": {},
            "outputs": [],
            "source": [
                "embedding_array = np.vstack([embeddings[word] for word in unique_tokens])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 209,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1\n",
                        "12133\n"
                    ]
                }
            ],
            "source": [
                "np.save(word_embeddings_file, embedding_array)\n",
                "print(len(embedding_array[max(word2id.values())]))\n",
                "print(max(word2id.values()))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 304,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "HParams object with values {'support_quick_scoring': True, 'dropout': 0.2, 'attention_hidden_dim': 200, 'head_num': 4, 'head_dim': 100, 'filter_num': 400, 'window_size': 3, 'vert_emb_dim': 100, 'subvert_emb_dim': 100, 'gru_unit': 400, 'type': 'ini', 'user_emb_dim': 50, 'learning_rate': 0.0001, 'optimizer': 'adam', 'epochs': 5, 'batch_size': 32, 'show_step': 100000, 'title_size': 30, 'his_size': 50, 'data_format': 'news', 'npratio': 4, 'metrics': ['group_auc', 'mean_mrr', 'ndcg@5;10'], 'word_emb_dim': 100, 'cnn_activation': 'relu', 'model_type': 'lstur', 'loss': 'cross_entropy_loss', 'wordEmb_file': '/Users/maxkleinegger/Downloads/tmp/word_embeddings.npy', 'wordDict_file': '/Users/maxkleinegger/Downloads/tmp/words_dict.pkl', 'userDict_file': '/Users/maxkleinegger/Downloads/tmp/user_dict.pkl'}\n"
                    ]
                }
            ],
            "source": [
                "hparams = prepare_hparams(yaml_file, \n",
                "                          wordEmb_file=word_embeddings_file,\n",
                "                          wordDict_file=words_dict_file, \n",
                "                          userDict_file=user_dict_file,\n",
                "                          batch_size=BATCH_SIZE,\n",
                "                          epochs=EPOCHS)\n",
                "print(hparams)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 305,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Copyright (c) Recommenders contributors.\n",
                "# Licensed under the MIT License.\n",
                "\n",
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "import pickle\n",
                "\n",
                "from recommenders.models.deeprec.io.iterator import BaseIterator\n",
                "from recommenders.models.newsrec.newsrec_utils import word_tokenize, newsample\n",
                "\n",
                "__all__ = [\"MINDIterator\"]\n",
                "\n",
                "\n",
                "class MINDIterator(BaseIterator):\n",
                "    \"\"\"Train data loader for NAML model.\n",
                "    The model require a special type of data format, where each instance contains a label, impresion id, user id,\n",
                "    the candidate news articles and user's clicked news article. Articles are represented by title words,\n",
                "    body words, verts and subverts.\n",
                "\n",
                "    Iterator will not load the whole data into memory. Instead, it loads data into memory\n",
                "    per mini-batch, so that large files can be used as input data.\n",
                "\n",
                "    Attributes:\n",
                "        col_spliter (str): column spliter in one line.\n",
                "        ID_spliter (str): ID spliter in one line.\n",
                "        batch_size (int): the samples num in one batch.\n",
                "        title_size (int): max word num in news title.\n",
                "        his_size (int): max clicked news num in user click history.\n",
                "        npratio (int): negaive and positive ratio used in negative sampling. -1 means no need of negtive sampling.\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(\n",
                "        self,\n",
                "        hparams,\n",
                "        npratio=-1,\n",
                "        col_spliter=\"\\t\",\n",
                "        ID_spliter=\"%\",\n",
                "    ):\n",
                "        \"\"\"Initialize an iterator. Create necessary placeholders for the model.\n",
                "\n",
                "        Args:\n",
                "            hparams (object): Global hyper-parameters. Some key setttings such as head_num and head_dim are there.\n",
                "            npratio (int): negaive and positive ratio used in negative sampling. -1 means no need of negtive sampling.\n",
                "            col_spliter (str): column spliter in one line.\n",
                "            ID_spliter (str): ID spliter in one line.\n",
                "        \"\"\"\n",
                "        self.col_spliter = col_spliter\n",
                "        self.ID_spliter = ID_spliter\n",
                "        self.batch_size = hparams.batch_size\n",
                "        self.title_size = hparams.title_size\n",
                "        self.his_size = hparams.his_size\n",
                "        self.npratio = npratio\n",
                "\n",
                "        self.word_dict = self.load_dict(hparams.wordDict_file)\n",
                "        self.uid2index = self.load_dict(hparams.userDict_file)\n",
                "\n",
                "    def load_dict(self, file_path):\n",
                "        \"\"\"load pickle file\n",
                "\n",
                "        Args:\n",
                "            file path (str): file path\n",
                "\n",
                "        Returns:\n",
                "            object: pickle loaded object\n",
                "        \"\"\"\n",
                "        with open(file_path, \"rb\") as f:\n",
                "            return pickle.load(f)\n",
                "\n",
                "    def init_news(self, news_file):\n",
                "        \"\"\"init news information given news file, such as news_title_index and nid2index.\n",
                "        Args:\n",
                "            news_file: path of news file\n",
                "        \"\"\"\n",
                "\n",
                "        self.nid2index = {}\n",
                "        news_title = [\"\"]\n",
                "\n",
                "        with tf.io.gfile.GFile(news_file, \"r\") as rd:\n",
                "            for line in rd:\n",
                "                nid, vert, title, ab, url = line.strip(\"\\n\").split(\n",
                "                    self.col_spliter\n",
                "                )\n",
                "                if nid in self.nid2index:\n",
                "                    continue\n",
                "\n",
                "                self.nid2index[nid] = len(self.nid2index) + 1\n",
                "                title = word_tokenize(title)\n",
                "                news_title.append(title)\n",
                "\n",
                "        self.news_title_index = np.zeros(\n",
                "            (len(news_title), self.title_size), dtype=\"int32\"\n",
                "        )\n",
                "\n",
                "        for news_index in range(len(news_title)):\n",
                "            title = news_title[news_index]\n",
                "            for word_index in range(min(self.title_size, len(title))):\n",
                "                if title[word_index] in self.word_dict:\n",
                "                    self.news_title_index[news_index, word_index] = self.word_dict[\n",
                "                        title[word_index].lower()\n",
                "                    ]\n",
                "\n",
                "    def init_behaviors(self, behaviors_file):\n",
                "        \"\"\"init behavior logs given behaviors file.\n",
                "\n",
                "        Args:\n",
                "        behaviors_file: path of behaviors file\n",
                "        \"\"\"\n",
                "        self.histories = []\n",
                "        self.imprs = []\n",
                "        self.labels = []\n",
                "        self.impr_indexes = []\n",
                "        self.uindexes = []\n",
                "\n",
                "        with tf.io.gfile.GFile(behaviors_file, \"r\") as rd:\n",
                "            impr_index = 0\n",
                "            for line in rd:\n",
                "                uid, time, history, impr = line.strip(\"\\n\").split(self.col_spliter)[-4:]\n",
                "\n",
                "                history = [self.nid2index[i[1:]] for i in history.split()]\n",
                "                history = [0] * (self.his_size - len(history)) + history[\n",
                "                    -self.his_size :\n",
                "                ]\n",
                "\n",
                "                impr_news = [self.nid2index[(i.split(\"-\")[0])[1:]] for i in impr.split()]\n",
                "                label = [int(i.split(\"-\")[1]) for i in impr.split()]\n",
                "                uindex = self.uid2index[uid] if uid in self.uid2index else 0\n",
                "\n",
                "                self.histories.append(history)\n",
                "                self.imprs.append(impr_news)\n",
                "                self.labels.append(label)\n",
                "                self.impr_indexes.append(impr_index)\n",
                "                self.uindexes.append(uindex)\n",
                "                impr_index += 1\n",
                "\n",
                "    def parser_one_line(self, line):\n",
                "        \"\"\"Parse one behavior sample into feature values.\n",
                "        if npratio is larger than 0, return negtive sampled result.\n",
                "\n",
                "        Args:\n",
                "            line (int): sample index.\n",
                "\n",
                "        Yields:\n",
                "            list: Parsed results including label, impression id , user id,\n",
                "            candidate_title_index, clicked_title_index.\n",
                "        \"\"\"\n",
                "        if self.npratio > 0:\n",
                "            impr_label = self.labels[line]\n",
                "            impr = self.imprs[line]\n",
                "\n",
                "            poss = []\n",
                "            negs = []\n",
                "\n",
                "            for news, click in zip(impr, impr_label):\n",
                "                if click == 1:\n",
                "                    poss.append(news)\n",
                "                else:\n",
                "                    negs.append(news)\n",
                "\n",
                "            for p in poss:\n",
                "                candidate_title_index = []\n",
                "                impr_index = []\n",
                "                user_index = []\n",
                "                label = [1] + [0] * self.npratio\n",
                "\n",
                "                n = newsample(negs, self.npratio)\n",
                "                candidate_title_index = self.news_title_index[[p] + n]\n",
                "                click_title_index = self.news_title_index[self.histories[line]]\n",
                "                impr_index.append(self.impr_indexes[line])\n",
                "                user_index.append(self.uindexes[line])\n",
                "\n",
                "                yield (\n",
                "                    label,\n",
                "                    impr_index,\n",
                "                    user_index,\n",
                "                    candidate_title_index,\n",
                "                    click_title_index,\n",
                "                )\n",
                "\n",
                "        else:\n",
                "            impr_label = self.labels[line]\n",
                "            impr = self.imprs[line]\n",
                "\n",
                "            for news, label in zip(impr, impr_label):\n",
                "                candidate_title_index = []\n",
                "                impr_index = []\n",
                "                user_index = []\n",
                "                label = [label]\n",
                "\n",
                "                candidate_title_index.append(self.news_title_index[news])\n",
                "                click_title_index = self.news_title_index[self.histories[line]]\n",
                "                impr_index.append(self.impr_indexes[line])\n",
                "                user_index.append(self.uindexes[line])\n",
                "\n",
                "                yield (\n",
                "                    label,\n",
                "                    impr_index,\n",
                "                    user_index,\n",
                "                    candidate_title_index,\n",
                "                    click_title_index,\n",
                "                )\n",
                "\n",
                "    def load_data_from_file(self, news_file, behavior_file):\n",
                "        \"\"\"Read and parse data from news file and behavior file.\n",
                "\n",
                "        Args:\n",
                "            news_file (str): A file contains several informations of news.\n",
                "            beahaviros_file (str): A file contains information of user impressions.\n",
                "\n",
                "        Yields:\n",
                "            object: An iterator that yields parsed results, in the format of dict.\n",
                "        \"\"\"\n",
                "\n",
                "        if not hasattr(self, \"news_title_index\"):\n",
                "            self.init_news(news_file)\n",
                "\n",
                "        if not hasattr(self, \"impr_indexes\"):\n",
                "            self.init_behaviors(behavior_file)\n",
                "\n",
                "        label_list = []\n",
                "        imp_indexes = []\n",
                "        user_indexes = []\n",
                "        candidate_title_indexes = []\n",
                "        click_title_indexes = []\n",
                "        cnt = 0\n",
                "\n",
                "        indexes = np.arange(len(self.labels))\n",
                "\n",
                "        if self.npratio > 0:\n",
                "            np.random.shuffle(indexes)\n",
                "\n",
                "        for index in indexes:\n",
                "            for (\n",
                "                label,\n",
                "                imp_index,\n",
                "                user_index,\n",
                "                candidate_title_index,\n",
                "                click_title_index,\n",
                "            ) in self.parser_one_line(index):\n",
                "                candidate_title_indexes.append(candidate_title_index)\n",
                "                click_title_indexes.append(click_title_index)\n",
                "                imp_indexes.append(imp_index)\n",
                "                user_indexes.append(user_index)\n",
                "                label_list.append(label)\n",
                "\n",
                "                cnt += 1\n",
                "                if cnt >= self.batch_size:\n",
                "                    yield self._convert_data(\n",
                "                        label_list,\n",
                "                        imp_indexes,\n",
                "                        user_indexes,\n",
                "                        candidate_title_indexes,\n",
                "                        click_title_indexes,\n",
                "                    )\n",
                "                    label_list = []\n",
                "                    imp_indexes = []\n",
                "                    user_indexes = []\n",
                "                    candidate_title_indexes = []\n",
                "                    click_title_indexes = []\n",
                "                    cnt = 0\n",
                "\n",
                "        if cnt > 0:\n",
                "            yield self._convert_data(\n",
                "                label_list,\n",
                "                imp_indexes,\n",
                "                user_indexes,\n",
                "                candidate_title_indexes,\n",
                "                click_title_indexes,\n",
                "            )\n",
                "\n",
                "    def _convert_data(\n",
                "        self,\n",
                "        label_list,\n",
                "        imp_indexes,\n",
                "        user_indexes,\n",
                "        candidate_title_indexes,\n",
                "        click_title_indexes,\n",
                "    ):\n",
                "        \"\"\"Convert data into numpy arrays that are good for further model operation.\n",
                "\n",
                "        Args:\n",
                "            label_list (list): a list of ground-truth labels.\n",
                "            imp_indexes (list): a list of impression indexes.\n",
                "            user_indexes (list): a list of user indexes.\n",
                "            candidate_title_indexes (list): the candidate news titles' words indices.\n",
                "            click_title_indexes (list): words indices for user's clicked news titles.\n",
                "\n",
                "        Returns:\n",
                "            dict: A dictionary, containing multiple numpy arrays that are convenient for further operation.\n",
                "        \"\"\"\n",
                "\n",
                "        labels = np.asarray(label_list, dtype=np.float32)\n",
                "        imp_indexes = np.asarray(imp_indexes, dtype=np.int32)\n",
                "        user_indexes = np.asarray(user_indexes, dtype=np.int32)\n",
                "        candidate_title_index_batch = np.asarray(\n",
                "            candidate_title_indexes, dtype=np.int64\n",
                "        )\n",
                "        click_title_index_batch = np.asarray(click_title_indexes, dtype=np.int64)\n",
                "        return {\n",
                "            \"impression_index_batch\": imp_indexes,\n",
                "            \"user_index_batch\": user_indexes,\n",
                "            \"clicked_title_batch\": click_title_index_batch,\n",
                "            \"candidate_title_batch\": candidate_title_index_batch,\n",
                "            \"labels\": labels,\n",
                "        }\n",
                "\n",
                "    def load_user_from_file(self, news_file, behavior_file):\n",
                "        \"\"\"Read and parse user data from news file and behavior file.\n",
                "\n",
                "        Args:\n",
                "            news_file (str): A file contains several informations of news.\n",
                "            beahaviros_file (str): A file contains information of user impressions.\n",
                "\n",
                "        Yields:\n",
                "            object: An iterator that yields parsed user feature, in the format of dict.\n",
                "        \"\"\"\n",
                "\n",
                "        if not hasattr(self, \"news_title_index\"):\n",
                "            self.init_news(news_file)\n",
                "\n",
                "        if not hasattr(self, \"impr_indexes\"):\n",
                "            self.init_behaviors(behavior_file)\n",
                "\n",
                "        user_indexes = []\n",
                "        impr_indexes = []\n",
                "        click_title_indexes = []\n",
                "        cnt = 0\n",
                "\n",
                "        for index in range(len(self.impr_indexes)):\n",
                "            click_title_indexes.append(self.news_title_index[self.histories[index]])\n",
                "            user_indexes.append(self.uindexes[index])\n",
                "            impr_indexes.append(self.impr_indexes[index])\n",
                "\n",
                "            cnt += 1\n",
                "            if cnt >= self.batch_size:\n",
                "                # print(self._convert_user_data(\n",
                "                #     user_indexes,\n",
                "                #     impr_indexes,\n",
                "                #     click_title_indexes,\n",
                "                # ))\n",
                "                yield self._convert_user_data(\n",
                "                    user_indexes,\n",
                "                    impr_indexes,\n",
                "                    click_title_indexes,\n",
                "                )\n",
                "                user_indexes = []\n",
                "                impr_indexes = []\n",
                "                click_title_indexes = []\n",
                "                cnt = 0\n",
                "\n",
                "        if cnt > 0:\n",
                "            yield self._convert_user_data(\n",
                "                user_indexes,\n",
                "                impr_indexes,\n",
                "                click_title_indexes,\n",
                "            )\n",
                "\n",
                "    def _convert_user_data(\n",
                "        self,\n",
                "        user_indexes,\n",
                "        impr_indexes,\n",
                "        click_title_indexes,\n",
                "    ):\n",
                "        \"\"\"Convert data into numpy arrays that are good for further model operation.\n",
                "\n",
                "        Args:\n",
                "            user_indexes (list): a list of user indexes.\n",
                "            click_title_indexes (list): words indices for user's clicked news titles.\n",
                "\n",
                "        Returns:\n",
                "            dict: A dictionary, containing multiple numpy arrays that are convenient for further operation.\n",
                "        \"\"\"\n",
                "\n",
                "        user_indexes = np.asarray(user_indexes, dtype=np.int32)\n",
                "        impr_indexes = np.asarray(impr_indexes, dtype=np.int32)\n",
                "        click_title_index_batch = np.asarray(click_title_indexes, dtype=np.int64)\n",
                "\n",
                "        return {\n",
                "            \"user_index_batch\": user_indexes,\n",
                "            \"impr_index_batch\": impr_indexes,\n",
                "            \"clicked_title_batch\": click_title_index_batch,\n",
                "        }\n",
                "\n",
                "    def load_news_from_file(self, news_file):\n",
                "        \"\"\"Read and parse user data from news file.\n",
                "\n",
                "        Args:\n",
                "            news_file (str): A file contains several informations of news.\n",
                "\n",
                "        Yields:\n",
                "            object: An iterator that yields parsed news feature, in the format of dict.\n",
                "        \"\"\"\n",
                "        if not hasattr(self, \"news_title_index\"):\n",
                "            self.init_news(news_file)\n",
                "\n",
                "        news_indexes = []\n",
                "        candidate_title_indexes = []\n",
                "        cnt = 0\n",
                "\n",
                "        for index in range(len(self.news_title_index)):\n",
                "            news_indexes.append(index)\n",
                "            candidate_title_indexes.append(self.news_title_index[index])\n",
                "\n",
                "            cnt += 1\n",
                "            if cnt >= self.batch_size:\n",
                "                yield self._convert_news_data(\n",
                "                    news_indexes,\n",
                "                    candidate_title_indexes,\n",
                "                )\n",
                "                news_indexes = []\n",
                "                candidate_title_indexes = []\n",
                "                cnt = 0\n",
                "\n",
                "\n",
                "        if cnt > 0:\n",
                "            yield self._convert_news_data(\n",
                "                news_indexes,\n",
                "                candidate_title_indexes,\n",
                "            )\n",
                "\n",
                "    def _convert_news_data(\n",
                "        self,\n",
                "        news_indexes,\n",
                "        candidate_title_indexes,\n",
                "    ):\n",
                "        \"\"\"Convert data into numpy arrays that are good for further model operation.\n",
                "\n",
                "        Args:\n",
                "            news_indexes (list): a list of news indexes.\n",
                "            candidate_title_indexes (list): the candidate news titles' words indices.\n",
                "\n",
                "        Returns:\n",
                "            dict: A dictionary, containing multiple numpy arrays that are convenient for further operation.\n",
                "        \"\"\"\n",
                "\n",
                "        news_indexes_batch = np.asarray(news_indexes, dtype=np.int32)\n",
                "        candidate_title_index_batch = np.asarray(\n",
                "            candidate_title_indexes, dtype=np.int32\n",
                "        )\n",
                "\n",
                "        return {\n",
                "            \"news_index_batch\": news_indexes_batch,\n",
                "            \"candidate_title_batch\": candidate_title_index_batch,\n",
                "        }\n",
                "\n",
                "    def load_impression_from_file(self, behaivors_file):\n",
                "        \"\"\"Read and parse impression data from behaivors file.\n",
                "\n",
                "        Args:\n",
                "            behaivors_file (str): A file contains several informations of behaviros.\n",
                "\n",
                "        Yields:\n",
                "            object: An iterator that yields parsed impression data, in the format of dict.\n",
                "        \"\"\"\n",
                "\n",
                "        if not hasattr(self, \"histories\"):\n",
                "            self.init_behaviors(behaivors_file)\n",
                "\n",
                "        indexes = np.arange(len(self.labels))\n",
                "\n",
                "        for index in indexes:\n",
                "            impr_label = np.array(self.labels[index], dtype=\"int32\")\n",
                "            impr_news = np.array(self.imprs[index], dtype=\"int32\")\n",
                "\n",
                "            yield (\n",
                "                self.impr_indexes[index],\n",
                "                impr_news,\n",
                "                self.uindexes[index],\n",
                "                impr_label,\n",
                "            )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 306,
            "metadata": {},
            "outputs": [],
            "source": [
                "iterator = MINDIterator"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Train the LSTUR model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 307,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2024-06-19 21:44:34.634132: W tensorflow/c/c_api.cc:305] Operation '{name:'embedding_50/embeddings/Assign' id:46277 op device:{requested: '', assigned: ''} def:{{{node embedding_50/embeddings/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](embedding_50/embeddings, embedding_50/embeddings/Initializer/stateless_random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tensor(\"conv1d_22/Relu:0\", shape=(None, 30, 400), dtype=float32)\n",
                        "Tensor(\"att_layer2_22/Sum_1:0\", shape=(None, 400), dtype=float32)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/maxkleinegger/Documents/TUWien/2024SS/RecSys/ue.nosync/Group_33/.venv/lib/python3.11/site-packages/keras/src/optimizers/legacy/adam.py:118: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
                        "  super().__init__(name, **kwargs)\n"
                    ]
                }
            ],
            "source": [
                "model = LSTURModel(hparams, iterator, seed=SEED)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 308,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "0it [00:00, ?it/s]/Users/maxkleinegger/Documents/TUWien/2024SS/RecSys/ue.nosync/Group_33/.venv/lib/python3.11/site-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
                        "  updates=self.state_updates,\n",
                        "2024-06-19 21:44:40.631393: W tensorflow/c/c_api.cc:305] Operation '{name:'conv1d_22/bias/Assign' id:46309 op device:{requested: '', assigned: ''} def:{{{node conv1d_22/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](conv1d_22/bias, conv1d_22/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
                        "649it [00:07, 89.76it/s] \n",
                        "0it [00:00, ?it/s]2024-06-19 21:44:45.374056: W tensorflow/c/c_api.cc:305] Operation '{name:'gru_22/strided_slice_2' id:47202 op device:{requested: '', assigned: ''} def:{{{node gru_22/strided_slice_2}} = StridedSlice[Index=DT_INT32, T=DT_FLOAT, _has_manual_control_dependencies=true, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](gru_22/TensorArrayV2Stack/TensorListStack, gru_22/strided_slice_2/stack, gru_22/strided_slice_2/stack_1, gru_22/strided_slice_2/stack_2)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
                        "77it [00:11,  6.81it/s]\n",
                        "2446it [00:00, 42231.37it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'group_auc': 0.5154, 'mean_mrr': 0.3221, 'ndcg@5': 0.3538, 'ndcg@10': 0.4407}\n"
                    ]
                }
            ],
            "source": [
                "print(model.run_eval(train_news_file, val_behaviors_file))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 309,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "0it [00:00, ?it/s]2024-06-19 21:45:03.119619: W tensorflow/c/c_api.cc:305] Operation '{name:'loss_22/mul' id:48299 op device:{requested: '', assigned: ''} def:{{{node loss_22/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_22/mul/x, loss_22/activation_44_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
                        "2024-06-19 21:45:05.579513: W tensorflow/c/c_api.cc:305] Operation '{name:'training_2/Adam/gru_22/gru_cell/bias/m/Assign' id:48916 op device:{requested: '', assigned: ''} def:{{{node training_2/Adam/gru_22/gru_cell/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_2/Adam/gru_22/gru_cell/bias/m, training_2/Adam/gru_22/gru_cell/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
                        "74it [00:42,  1.74it/s]\n",
                        "649it [00:01, 363.92it/s]\n",
                        "77it [00:10,  7.53it/s]\n",
                        "2446it [00:00, 20781.29it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "at epoch 1\n",
                        "train info: logloss loss:1.6152885620658461\n",
                        "eval info: group_auc:0.546, mean_mrr:0.3452, ndcg@10:0.4607, ndcg@5:0.3835\n",
                        "at epoch 1 , train time: 42.5 eval time: 13.2\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "74it [00:36,  2.02it/s]\n",
                        "649it [00:02, 302.86it/s]\n",
                        "77it [00:09,  7.70it/s]\n",
                        "2446it [00:00, 4348.66it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "at epoch 2\n",
                        "train info: logloss loss:1.5847850651354403\n",
                        "eval info: group_auc:0.5428, mean_mrr:0.3417, ndcg@10:0.4585, ndcg@5:0.3785\n",
                        "at epoch 2 , train time: 36.6 eval time: 13.8\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "74it [00:36,  2.04it/s]\n",
                        "649it [00:01, 370.87it/s]\n",
                        "77it [00:10,  7.43it/s]\n",
                        "2446it [00:00, 5481.42it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "at epoch 3\n",
                        "train info: logloss loss:1.5530094182169116\n",
                        "eval info: group_auc:0.5381, mean_mrr:0.3381, ndcg@10:0.456, ndcg@5:0.3763\n",
                        "at epoch 3 , train time: 36.2 eval time: 13.6\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "74it [00:35,  2.07it/s]\n",
                        "649it [00:01, 373.28it/s]\n",
                        "77it [00:09,  7.84it/s]\n",
                        "2446it [00:00, 13926.87it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "at epoch 4\n",
                        "train info: logloss loss:1.539530462509877\n",
                        "eval info: group_auc:0.5345, mean_mrr:0.335, ndcg@10:0.4521, ndcg@5:0.3731\n",
                        "at epoch 4 , train time: 35.8 eval time: 12.8\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "74it [00:35,  2.08it/s]\n",
                        "649it [00:02, 311.29it/s]\n",
                        "77it [00:10,  7.50it/s]\n",
                        "2446it [00:00, 21643.75it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "at epoch 5\n",
                        "train info: logloss loss:1.5066116033373653\n",
                        "eval info: group_auc:0.5429, mean_mrr:0.34, ndcg@10:0.4578, ndcg@5:0.3789\n",
                        "at epoch 5 , train time: 35.6 eval time: 13.6\n",
                        "CPU times: user 16min 11s, sys: 2min 34s, total: 18min 45s\n",
                        "Wall time: 4min 13s\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "<recommenders.models.newsrec.models.lstur.LSTURModel at 0x357090090>"
                        ]
                    },
                    "execution_count": 309,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "%%time\n",
                "model.fit(train_news_file, train_behaviors_file, train_news_file, val_behaviors_file)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 311,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "649it [00:01, 376.64it/s]\n",
                        "77it [00:09,  8.07it/s]\n",
                        "2446it [00:00, 15752.11it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{'group_auc': 0.5429, 'mean_mrr': 0.34, 'ndcg@5': 0.3789, 'ndcg@10': 0.4578}\n",
                        "CPU times: user 45.1 s, sys: 5.72 s, total: 50.8 s\n",
                        "Wall time: 12.5 s\n"
                    ]
                }
            ],
            "source": [
                "res_syn = model.run_eval(articles_file, val_behaviors_file)\n",
                "print(res_syn)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 312,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/notebook_utils.json+json": {
                            "data": 0.5429,
                            "encoder": "json",
                            "name": "group_auc"
                        }
                    },
                    "metadata": {
                        "notebook_utils": {
                            "data": true,
                            "display": false,
                            "name": "group_auc"
                        }
                    },
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/notebook_utils.json+json": {
                            "data": 0.34,
                            "encoder": "json",
                            "name": "mean_mrr"
                        }
                    },
                    "metadata": {
                        "notebook_utils": {
                            "data": true,
                            "display": false,
                            "name": "mean_mrr"
                        }
                    },
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/notebook_utils.json+json": {
                            "data": 0.3789,
                            "encoder": "json",
                            "name": "ndcg@5"
                        }
                    },
                    "metadata": {
                        "notebook_utils": {
                            "data": true,
                            "display": false,
                            "name": "ndcg@5"
                        }
                    },
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/notebook_utils.json+json": {
                            "data": 0.4578,
                            "encoder": "json",
                            "name": "ndcg@10"
                        }
                    },
                    "metadata": {
                        "notebook_utils": {
                            "data": true,
                            "display": false,
                            "name": "ndcg@10"
                        }
                    },
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# Record results for tests - ignore this cell\n",
                "store_metadata(\"group_auc\", res_syn['group_auc'])\n",
                "store_metadata(\"mean_mrr\", res_syn['mean_mrr'])\n",
                "store_metadata(\"ndcg@5\", res_syn['ndcg@5'])\n",
                "store_metadata(\"ndcg@10\", res_syn['ndcg@10'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Save the model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_path = os.path.join(PATH, \"model\")\n",
                "os.makedirs(model_path, exist_ok=True)\n",
                "\n",
                "model.model.save_weights(os.path.join(model_path, \"lstur_native_model\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Output Prediction File\n",
                "This code segment is used to generate the prediction.zip file, which is in the same format in [MIND Competition Submission Tutorial](https://competitions.codalab.org/competitions/24122#learn_the_details-submission-guidelines).\n",
                "\n",
                "Please change the `MIND_type` parameter to `large` if you want to submit your prediction to [MIND Competition](https://msnews.github.io/competition.html)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "group_impr_indexes, group_labels, group_preds = model.run_fast_eval(valid_news_file, valid_behaviors_file)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(os.path.join(data_path, 'prediction.txt'), 'w') as f:\n",
                "    for impr_index, preds in tqdm(zip(group_impr_indexes, group_preds)):\n",
                "        impr_index += 1\n",
                "        pred_rank = (np.argsort(np.argsort(preds)[::-1]) + 1).tolist()\n",
                "        pred_rank = '[' + ','.join([str(i) for i in pred_rank]) + ']'\n",
                "        f.write(' '.join([str(impr_index), pred_rank])+ '\\n')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "f = zipfile.ZipFile(os.path.join(data_path, 'prediction.zip'), 'w', zipfile.ZIP_DEFLATED)\n",
                "f.write(os.path.join(data_path, 'prediction.txt'), arcname='prediction.txt')\n",
                "f.close()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Reference\n",
                "\\[1\\] Mingxiao An, Fangzhao Wu, Chuhan Wu, Kun Zhang, Zheng Liu and Xing Xie: Neural News Recommendation with Long- and Short-term User Representations, ACL 2019<br>\n",
                "\\[2\\] Wu, Fangzhao, et al. \"MIND: A Large-scale Dataset for News Recommendation\" Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. https://msnews.github.io/competition.html <br>\n",
                "\\[3\\] GloVe: Global Vectors for Word Representation. https://nlp.stanford.edu/projects/glove/"
            ]
        }
    ],
    "metadata": {
        "celltoolbar": "Tags",
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
