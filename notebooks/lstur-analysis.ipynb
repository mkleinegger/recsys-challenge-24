{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33735d69-9b06-4ec4-8860-99c3e42bebb8",
   "metadata": {},
   "source": [
    "# LSTUR: Analysis of Hyperparameter Optimization Results\n",
    "This notebook provides analysis of the results obtained from the hyperparameter optimization for the LSTUR model, focusing on the effectiveness of different parameter configurations. The primary goal is to visualize and quantify the impact of each hyperparameter on the model's performance metrics.\n",
    "\n",
    "LSTUR \\[2\\] is a news recommendation approach that captures users' both long-term preferences and short-term interests. The core of LSTUR is composed of a news encoder and a user encoder. The news encoder learns representations of news from their titles, while the user encoder learns long-term user representations from the embeddings of their IDs and short-term user representations from their recently browsed news via a GRU network.\n",
    "\n",
    "## Properties of LSTUR:\n",
    "\n",
    "- **Dual User Representations**: LSTUR captures both short-term and long-term preferences by using embeddings of users' IDs for long-term user representations and a GRU network to learn short-term user representations from recently browsed articles.\n",
    "- **News Encoder**: Utilizes the news titles to generate news representations.\n",
    "- **User Encoder**: Combines long-term and short-term user representations. Two methods are proposed for this combination:\n",
    "  - Initializing the hidden state of the GRU network with the long-term user representation.\n",
    "  - Concatenating both long-term and short-term user representations to form a unified user vector.\n",
    " \n",
    "\\[1\\] https://github.com/ebanalyse/ebnerd-benchmark\n",
    "\n",
    "\\[2\\] https://aclanthology.org/P19-1033/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f03aab-a935-486c-ab64-e5abbf56e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcef84a8-7737-4166-95a0-403c5b73528d",
   "metadata": {},
   "source": [
    "## Hyperparameter Grid Definition\n",
    "This cell initializes the `param_grid` dictionary containing potential values for different hyperparameters of the LSTUR model. It then generates a list of all possible combinations of these parameters using a Cartesian product. This setup is essential for the subsequent steps of hyperparameter optimization, where each combination will be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73301eff-80cf-4e43-a3d3-b92df88dec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'history_size': [10, 50, 100],\n",
    "    'n_users': [20000, 50000, 70000],\n",
    "    'title_size': [10, 50, 100],\n",
    "    'learning_rate': [0.0001, 0.001, 0.01],\n",
    "    'dropout':  [0.1, 0.3, 0.5]\n",
    "}\n",
    "\n",
    "combinations = list(itertools.product(\n",
    "    param_grid['history_size'],\n",
    "    param_grid['n_users'],\n",
    "    param_grid['title_size'],\n",
    "    param_grid['learning_rate'],\n",
    "    param_grid['dropout']\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a92e37-e568-43e6-9877-ab4aff3c9189",
   "metadata": {},
   "source": [
    "## Model Evaluation Results Processing\n",
    "This cell iterates through all combinations of hyperparameters generated previously to collect the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aeab8d-03ea-43fb-8a79-f0dbc3f303a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for history_size, n_users, title_size, learning_rate, dropout in combinations:\n",
    "    MODEL_NAME = f\"LSTUR_h{history_size}_u{n_users}_t{title_size}_l{learning_rate}_d{dropout}\"\n",
    "    RESULTS_DIR = f\"../output/evaluations/{MODEL_NAME}\"\n",
    "    results_file = os.path.join(RESULTS_DIR, 'evaluation_results.txt')\n",
    "    \n",
    "    if os.path.exists(results_file):\n",
    "        with open(results_file, 'r') as file:\n",
    "            # Read the results from the file\n",
    "            eval_results = {}\n",
    "            for line in file:\n",
    "                key, value = line.strip().split(': ')\n",
    "                eval_results[key] = float(value)\n",
    "            \n",
    "            # Add the hyperparameters to the results\n",
    "            eval_results['history_size'] = history_size\n",
    "            eval_results['n_users'] = n_users\n",
    "            eval_results['title_size'] = title_size\n",
    "            eval_results['learning_rate'] = learning_rate\n",
    "            eval_results['dropout'] = dropout\n",
    "            \n",
    "            \n",
    "            # Append to the results list\n",
    "            results.append(eval_results)\n",
    "    else:\n",
    "        print(f\"Results file not found for combination: history_size={history_size}, n_users={n_users}, title_size={title_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a578fb17-e2cd-4fa1-9cc1-f59c51589a36",
   "metadata": {},
   "source": [
    "# Visualization and Analysis of Model Performance Metrics\n",
    "\n",
    "The following cells contain code for visualizing and analyzing the model performance based on the collected hyperparameter evaluation results. We use heatmaps to provide a clear visual representation of how different hyperparameter combinations and specific parameters like dropout rates affect model accuracy metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cc96f6-3bb5-4def-8bf9-ce3cd32510ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e757d-3ea5-4184-9fba-acc6364a733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['combination'] = df.apply(lambda row: f\"h{row['history_size']}_u{row['n_users']}_t{row['title_size']}\", axis=1)\n",
    "df['combination'] = df.apply(lambda row: f\"l{row['learning_rate']}_d{row['dropout']}\", axis=1)\n",
    "\n",
    "# Select only the relevant columns for the heatmap\n",
    "heatmap_data = df[['combination', 'auc', 'mrr', 'ndcg@5', 'ndcg@10']]\n",
    "\n",
    "# Set the combination column as the index\n",
    "heatmap_data.set_index('combination', inplace=True)\n",
    "\n",
    "# Normalize each metric individually\n",
    "scaler = MinMaxScaler()\n",
    "heatmap_data[['auc', 'mrr', 'ndcg@5', 'ndcg@10']] = scaler.fit_transform(heatmap_data[['auc', 'mrr', 'ndcg@5', 'ndcg@10']])\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".4f\", cbar=True)\n",
    "plt.title('Heatmap of Accuracy Metrics for Different Hyperparameter Combinations')\n",
    "plt.ylabel('Hyperparameter Combinations')\n",
    "plt.xlabel('Accuracy Metrics')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47b1122-682b-4a7d-9577-a58be858d123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by dropout and calculate the mean of each metric\n",
    "mean_results = df.groupby('dropout').mean().reset_index()\n",
    "\n",
    "# Select only the relevant columns for the heatmap\n",
    "heatmap_data = mean_results[['dropout', 'auc', 'mrr', 'ndcg@5', 'ndcg@10']]\n",
    "\n",
    "# Set the dropout column as the index\n",
    "heatmap_data.set_index('dropout', inplace=True)\n",
    "\n",
    "# Normalize each metric individually\n",
    "scaler = MinMaxScaler()\n",
    "heatmap_data[['auc', 'mrr', 'ndcg@5', 'ndcg@10']] = scaler.fit_transform(heatmap_data[['auc', 'mrr', 'ndcg@5', 'ndcg@10']])\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".4f\", cmap=\"YlGnBu\", cbar=True)\n",
    "plt.title('Heatmap of Normalized Accuracy Metrics for Different Dropout Rates')\n",
    "plt.ylabel('Dropout Rate')\n",
    "plt.xlabel('Accuracy Metrics')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
