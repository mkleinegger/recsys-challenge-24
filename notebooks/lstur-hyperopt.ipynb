{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "In this notebook, we illustrate how to use the Neural News Recommendation with Multi-Head Self-Attention ([NRMS](https://aclanthology.org/D19-1671/)). The implementation is taken from the [recommenders](https://github.com/recommenders-team/recommenders) repository. We have simply stripped the model to keep it cleaner.\n",
    "\n",
    "We use a small dataset, which is downloaded from [recsys.eb.dk](https://recsys.eb.dk/). All the datasets are stored in the folder path ```~/ebnerd_data/*```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 18:38:29.079721: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-19 18:38:29.079814: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-19 18:38:29.083315: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-19 18:38:29.102984: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-19 18:38:30.607070: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "import shutil\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "from ebrec.utils._constants import (\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_SUBTITLE_COL,\n",
    "    DEFAULT_LABELS_COL,\n",
    "    DEFAULT_TITLE_COL,\n",
    "    DEFAULT_USER_COL,\n",
    ")\n",
    "\n",
    "from ebrec.utils._behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    "    add_known_user_column,\n",
    "    add_prediction_scores,\n",
    "    truncate_history,\n",
    ")\n",
    "from ebrec.utils._articles import convert_text2encoding_with_transformers\n",
    "from ebrec.utils._polars import concat_str_columns, slice_join_dataframes\n",
    "from ebrec.utils._articles import create_article_id_to_value_mapping\n",
    "from ebrec.utils._nlp import get_transformers_word_embeddings\n",
    "from ebrec.utils._python import write_submission_file, rank_predictions_by_score\n",
    "\n",
    "from ebrec.models.newsrec.dataloader import LSTURDataLoader\n",
    "from ebrec.models.newsrec.model_config import hparams_lstur\n",
    "from ebrec.models.newsrec import LSTURModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ebnerd_from_path(path: Path, history_size: int = 30) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load ebnerd - function\n",
    "    \"\"\"\n",
    "    df_history = (\n",
    "        pl.scan_parquet(path.joinpath(\"history.parquet\"))\n",
    "        .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "        .pipe(\n",
    "            truncate_history,\n",
    "            column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "            history_size=history_size,\n",
    "            padding_value=0,\n",
    "            enable_warning=False,\n",
    "        )\n",
    "    )\n",
    "    df_behaviors = (\n",
    "        pl.scan_parquet(path.joinpath(\"behaviors.parquet\"))\n",
    "        .collect()\n",
    "        .pipe(\n",
    "            slice_join_dataframes,\n",
    "            df2=df_history.collect(),\n",
    "            on=DEFAULT_USER_COL,\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "    return df_behaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate labels\n",
    "We sample a few just to get started. For testset we just make up a dummy column with 0 and 1 - this is not the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path(\"../data/\")\n",
    "DATASPLIT = \"ebnerd_small\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we sample the dataset, just to keep it smaller. Also, one can simply add the testset similary to the validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i64]</td><td>list[i64]</td><td>u32</td><td>list[i8]</td></tr></thead><tbody><tr><td>2433383</td><td>[9762288, 9770400, â€¦ 9770327]</td><td>[7420751, 9774187, â€¦ 7420751]</td><td>[9774187]</td><td>451574915</td><td>[0, 1, â€¦ 0]</td></tr><tr><td>1332809</td><td>[9768819, 9768829, â€¦ 9770146]</td><td>[9770082, 9771796, â€¦ 9771796]</td><td>[9770082]</td><td>375736276</td><td>[1, 0, â€¦ 0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 6)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ user_id â”† article_id_fixed  â”† article_ids_invie â”† article_ids_clic â”† impression_id â”† labels      â”‚\n",
       "â”‚ ---     â”† ---               â”† w                 â”† ked              â”† ---           â”† ---         â”‚\n",
       "â”‚ u32     â”† list[i32]         â”† ---               â”† ---              â”† u32           â”† list[i8]    â”‚\n",
       "â”‚         â”†                   â”† list[i64]         â”† list[i64]        â”†               â”†             â”‚\n",
       "â•žâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 2433383 â”† [9762288,         â”† [7420751,         â”† [9774187]        â”† 451574915     â”† [0, 1, â€¦ 0] â”‚\n",
       "â”‚         â”† 9770400, â€¦        â”† 9774187, â€¦        â”†                  â”†               â”†             â”‚\n",
       "â”‚         â”† 9770327]          â”† 7420751]          â”†                  â”†               â”†             â”‚\n",
       "â”‚ 1332809 â”† [9768819,         â”† [9770082,         â”† [9770082]        â”† 375736276     â”† [1, 0, â€¦ 0] â”‚\n",
       "â”‚         â”† 9768829, â€¦        â”† 9771796, â€¦        â”†                  â”†               â”†             â”‚\n",
       "â”‚         â”† 9770146]          â”† 9771796]          â”†                  â”†               â”†             â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "]\n",
    "HISTORY_SIZE = 10\n",
    "FRACTION = 0.01\n",
    "\n",
    "df_train = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"train\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=4,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=123,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "# =>\n",
    "df_validation = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"validation\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>article_id</th><th>title</th><th>subtitle</th><th>last_modified_time</th><th>premium</th><th>body</th><th>published_time</th><th>image_ids</th><th>article_type</th><th>url</th><th>ner_clusters</th><th>entity_groups</th><th>topics</th><th>category</th><th>subcategory</th><th>category_str</th><th>total_inviews</th><th>total_pageviews</th><th>total_read_time</th><th>sentiment_score</th><th>sentiment_label</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>datetime[Î¼s]</td><td>bool</td><td>str</td><td>datetime[Î¼s]</td><td>list[i64]</td><td>str</td><td>str</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>i16</td><td>list[i16]</td><td>str</td><td>i32</td><td>i32</td><td>f32</td><td>f32</td><td>str</td></tr></thead><tbody><tr><td>3001353</td><td>&quot;Natascha var iâ€¦</td><td>&quot;Politiet frygtâ€¦</td><td>2023-06-29 06:20:33</td><td>false</td><td>&quot;Sagen om den Ã¸â€¦</td><td>2006-08-31 08:06:45</td><td>[3150850]</td><td>&quot;article_defaulâ€¦</td><td>&quot;https://ekstraâ€¦</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Personfarlig kriminalitet&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9955</td><td>&quot;Negative&quot;</td></tr><tr><td>3003065</td><td>&quot;Kun Star Wars â€¦</td><td>&quot;BiografgÃ¦ngernâ€¦</td><td>2023-06-29 06:20:35</td><td>false</td><td>&quot;Vatikanet har â€¦</td><td>2006-05-21 16:57:00</td><td>[3006712]</td><td>&quot;article_defaulâ€¦</td><td>&quot;https://ekstraâ€¦</td><td>[]</td><td>[]</td><td>[&quot;Underholdning&quot;, &quot;Film og tv&quot;, &quot;Ã˜konomi&quot;]</td><td>414</td><td>[433, 434]</td><td>&quot;underholdning&quot;</td><td>null</td><td>null</td><td>null</td><td>0.846</td><td>&quot;Positive&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 21)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ article_i â”† title     â”† subtitle  â”† last_modi â”† â€¦ â”† total_pag â”† total_rea â”† sentiment â”† sentimen â”‚\n",
       "â”‚ d         â”† ---       â”† ---       â”† fied_time â”†   â”† eviews    â”† d_time    â”† _score    â”† t_label  â”‚\n",
       "â”‚ ---       â”† str       â”† str       â”† ---       â”†   â”† ---       â”† ---       â”† ---       â”† ---      â”‚\n",
       "â”‚ i32       â”†           â”†           â”† datetime[ â”†   â”† i32       â”† f32       â”† f32       â”† str      â”‚\n",
       "â”‚           â”†           â”†           â”† Î¼s]       â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â•žâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 3001353   â”† Natascha  â”† Politiet  â”† 2023-06-2 â”† â€¦ â”† null      â”† null      â”† 0.9955    â”† Negative â”‚\n",
       "â”‚           â”† var ikke  â”† frygter   â”† 9         â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚           â”† den       â”† nu, at    â”† 06:20:33  â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚           â”† fÃ¸rste    â”† Nataschaâ€¦ â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚ 3003065   â”† Kun Star  â”† BiografgÃ¦ â”† 2023-06-2 â”† â€¦ â”† null      â”† null      â”† 0.846     â”† Positive â”‚\n",
       "â”‚           â”† Wars      â”† ngerne    â”† 9         â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚           â”† tjente    â”† strÃ¸mmer  â”† 06:20:35  â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â”‚           â”† mere      â”† ind forâ€¦  â”†           â”†   â”†           â”†           â”†           â”†          â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles = pl.read_parquet(PATH.joinpath(DATASPLIT, \"articles.parquet\"))\n",
    "df_articles.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model using HuggingFace's tokenizer and wordembedding\n",
    "In the original implementation, they use the GloVe embeddings and tokenizer. To get going fast, we'll use a multilingual LLM from Hugging Face. \n",
    "Utilizing the tokenizer to tokenize the articles and the word-embedding to init NRMS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtaha/recsys/group-33-8Iui3YMO-py3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "TEXT_COLUMNS_TO_USE = [DEFAULT_SUBTITLE_COL, DEFAULT_TITLE_COL]\n",
    "MAX_TITLE_LENGTH = 50\n",
    "\n",
    "# LOAD HUGGINGFACE:\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "# We'll init the word embeddings using the\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "#\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "    df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH\n",
    ")\n",
    "# =>\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=token_col_title\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate the dataloaders\n",
    "In the implementations we have disconnected the models and data. Hence, you should built a dataloader that fits your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_mapping = {user_id: i for i, user_id in enumerate(df_train[DEFAULT_USER_COL].unique())}\n",
    "\n",
    "train_dataloader = LSTURDataLoader(\n",
    "    user_id_mapping=user_id_mapping,\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=64,\n",
    ")\n",
    "val_dataloader = LSTURDataLoader(\n",
    "    user_id_mapping=user_id_mapping,\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=True,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Hyperopt\n",
    "First we perform hyperopt in input dimensions namely: hoistory_size, user_size and title_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(history_size, n_users, title_size, learning_rate, dropout, df_validation,df_train):\n",
    "\n",
    "    # Create directories for logs and model weights\n",
    "    MODEL_NAME = f\"LSTUR_l{learning_rate}_d{dropout}\"\n",
    "    LOG_DIR = f\"downloads/runs/{MODEL_NAME}\"\n",
    "    MODEL_WEIGHTS = f\"downloads/data/state_dict/{MODEL_NAME}/weights\"\n",
    "    RESULTS_DIR = f\"downloads/evaluations/{MODEL_NAME}\"\n",
    "    \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=1)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2)\n",
    "    modelcheckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=MODEL_WEIGHTS, save_best_only=True, save_weights_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    # Set the parameters\n",
    "    hparams_lstur.history_size = history_size\n",
    "    hparams_lstur.n_users = n_users\n",
    "    hparams_lstur.title_size = title_size\n",
    "    hparams_lstur.learning_rate = learning_rate\n",
    "    hparams_lstur.dropout = dropout\n",
    "    \n",
    "\n",
    "    model = LSTURModel(\n",
    "        hparams=hparams_lstur,\n",
    "        word2vec_embedding=word2vec_embedding,\n",
    "        seed=42,\n",
    "    )\n",
    "    hist = model.model.fit(\n",
    "        train_dataloader,\n",
    "        validation_data=val_dataloader,\n",
    "        epochs=1,\n",
    "        callbacks=[tensorboard_callback, early_stopping, modelcheckpoint],\n",
    "    )\n",
    "    _ = model.model.load_weights(filepath=MODEL_WEIGHTS)\n",
    "\n",
    "\n",
    "    \n",
    "    pred_validation = model.scorer.predict(val_dataloader)\n",
    "    df_validation = add_prediction_scores(df_validation, pred_validation.tolist()).pipe(\n",
    "        add_known_user_column, known_users=df_train[DEFAULT_USER_COL]\n",
    "    )\n",
    "\n",
    "    from ebrec.evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "    \n",
    "    metrics = MetricEvaluator(\n",
    "        labels=df_validation[\"labels\"].to_list(),\n",
    "        predictions=df_validation[\"scores\"].to_list(),\n",
    "        metric_functions=[AucScore(), MrrScore(), NdcgScore(k=5), NdcgScore(k=10)],\n",
    "    )\n",
    "    evaluation_results = metrics.evaluate().evaluations\n",
    "\n",
    "    # Save the evaluation results\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "    with open(os.path.join(RESULTS_DIR, 'evaluation_results.txt'), 'w') as f:\n",
    "        for key, value in evaluation_results.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "    # Rank predictions and write submission file\n",
    "    df_validation = df_validation.with_columns(\n",
    "        pl.col(\"scores\")\n",
    "        .map_elements(lambda x: list(rank_predictions_by_score(x)))\n",
    "        .alias(\"ranked_scores\")\n",
    "    )\n",
    "    write_submission_file(\n",
    "        impression_ids=df_validation[DEFAULT_IMPRESSION_ID_COL],\n",
    "        prediction_scores=df_validation[\"ranked_scores\"],\n",
    "        path=os.path.join(RESULTS_DIR, \"predictions.txt\"),\n",
    "    )\n",
    "\n",
    "    return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating combination: history_size=100, n_users=70000, title_size=50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 18:39:03.339396: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-19 18:39:03.345001: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-19 18:39:03.348078: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-19 18:39:03.352046: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-19 18:39:03.355090: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-19 18:39:03.358175: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-19 18:39:03.635084: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-19 18:39:03.637109: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-19 18:39:03.638990: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-19 18:39:03.640827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13775 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:06:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 400), dtype=tf.float32, name=None), name='att_layer2/Sum_1:0', description=\"created by layer 'att_layer2'\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 18:39:07.941617: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-06-19 18:39:15.400662: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-19 18:39:15.670208: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-06-19 18:39:18.300474: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f7bd5fac8f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-19 18:39:18.300548: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2024-06-19 18:39:18.316275: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1718822358.523325   11580 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - ETA: 0s - loss: 1.5978"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 18:44:06.538086: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 46080368640 exceeds 10% of free system memory.\n",
      "2024-06-19 18:44:35.844144: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 46080368640 exceeds 10% of free system memory.\n",
      "2024-06-19 18:45:02.633908: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 6720096000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.00000, saving model to downloads/data/state_dict/LSTUR_l0.0001_d0.1/weights\n",
      "37/37 [==============================] - 363s 10s/step - loss: 1.5978 - val_loss: 0.0000e+00\n",
      "77/77 [==============================] - 14s 161ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2446it [00:00, 10941.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping downloads/evaluations/LSTUR_l0.0001_d0.1/predictions.txt to downloads/evaluations/LSTUR_l0.0001_d0.1/predictions.zip\n",
      "Evaluating combination: history_size=100, n_users=70000, title_size=50\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 400), dtype=tf.float32, name=None), name='att_layer2_1/Sum_1:0', description=\"created by layer 'att_layer2_1'\")\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.5979"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 18:46:10.737159: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 46080368640 exceeds 10% of free system memory.\n",
      "2024-06-19 18:46:40.059043: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 46080368640 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.00000, saving model to downloads/data/state_dict/LSTUR_l0.0001_d0.3/weights\n",
      "37/37 [==============================] - 98s 3s/step - loss: 1.5979 - val_loss: 0.0000e+00\n",
      "77/77 [==============================] - 14s 160ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2446it [00:00, 11404.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping downloads/evaluations/LSTUR_l0.0001_d0.3/predictions.txt to downloads/evaluations/LSTUR_l0.0001_d0.3/predictions.zip\n",
      "Evaluating combination: history_size=100, n_users=70000, title_size=50\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 400), dtype=tf.float32, name=None), name='att_layer2_2/Sum_1:0', description=\"created by layer 'att_layer2_2'\")\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.6087\n",
      "Epoch 1: val_loss improved from inf to 0.00000, saving model to downloads/data/state_dict/LSTUR_l0.0001_d0.5/weights\n",
      "37/37 [==============================] - 97s 3s/step - loss: 1.6087 - val_loss: 0.0000e+00\n",
      "77/77 [==============================] - 14s 165ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2446it [00:00, 11361.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping downloads/evaluations/LSTUR_l0.0001_d0.5/predictions.txt to downloads/evaluations/LSTUR_l0.0001_d0.5/predictions.zip\n",
      "Evaluating combination: history_size=100, n_users=70000, title_size=50\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 400), dtype=tf.float32, name=None), name='att_layer2_3/Sum_1:0', description=\"created by layer 'att_layer2_3'\")\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.5879\n",
      "Epoch 1: val_loss improved from inf to 0.00000, saving model to downloads/data/state_dict/LSTUR_l0.001_d0.1/weights\n",
      "37/37 [==============================] - 100s 3s/step - loss: 1.5879 - val_loss: 0.0000e+00\n",
      "77/77 [==============================] - 14s 159ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2446it [00:00, 11382.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping downloads/evaluations/LSTUR_l0.001_d0.1/predictions.txt to downloads/evaluations/LSTUR_l0.001_d0.1/predictions.zip\n",
      "Evaluating combination: history_size=100, n_users=70000, title_size=50\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 400), dtype=tf.float32, name=None), name='att_layer2_4/Sum_1:0', description=\"created by layer 'att_layer2_4'\")\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.6053\n",
      "Epoch 1: val_loss improved from inf to 0.00000, saving model to downloads/data/state_dict/LSTUR_l0.001_d0.3/weights\n",
      "37/37 [==============================] - 98s 2s/step - loss: 1.6053 - val_loss: 0.0000e+00\n",
      "77/77 [==============================] - 14s 159ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2446it [00:00, 11402.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping downloads/evaluations/LSTUR_l0.001_d0.3/predictions.txt to downloads/evaluations/LSTUR_l0.001_d0.3/predictions.zip\n",
      "Evaluating combination: history_size=100, n_users=70000, title_size=50\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 400), dtype=tf.float32, name=None), name='att_layer2_5/Sum_1:0', description=\"created by layer 'att_layer2_5'\")\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.6184\n",
      "Epoch 1: val_loss improved from inf to 0.00000, saving model to downloads/data/state_dict/LSTUR_l0.001_d0.5/weights\n",
      "37/37 [==============================] - 99s 3s/step - loss: 1.6184 - val_loss: 0.0000e+00\n",
      "77/77 [==============================] - 14s 158ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2446it [00:00, 11446.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping downloads/evaluations/LSTUR_l0.001_d0.5/predictions.txt to downloads/evaluations/LSTUR_l0.001_d0.5/predictions.zip\n",
      "Evaluating combination: history_size=100, n_users=70000, title_size=50\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 400), dtype=tf.float32, name=None), name='att_layer2_6/Sum_1:0', description=\"created by layer 'att_layer2_6'\")\n",
      "37/37 [==============================] - ETA: 0s - loss: 4.1866\n",
      "Epoch 1: val_loss improved from inf to 0.00000, saving model to downloads/data/state_dict/LSTUR_l0.01_d0.1/weights\n",
      "37/37 [==============================] - 98s 3s/step - loss: 4.1866 - val_loss: 0.0000e+00\n",
      "77/77 [==============================] - 14s 159ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2446it [00:00, 11311.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping downloads/evaluations/LSTUR_l0.01_d0.1/predictions.txt to downloads/evaluations/LSTUR_l0.01_d0.1/predictions.zip\n",
      "Evaluating combination: history_size=100, n_users=70000, title_size=50\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 400), dtype=tf.float32, name=None), name='att_layer2_7/Sum_1:0', description=\"created by layer 'att_layer2_7'\")\n",
      "37/37 [==============================] - ETA: 0s - loss: 3.4180\n",
      "Epoch 1: val_loss improved from inf to 0.00000, saving model to downloads/data/state_dict/LSTUR_l0.01_d0.3/weights\n",
      "37/37 [==============================] - 98s 3s/step - loss: 3.4180 - val_loss: 0.0000e+00\n",
      "77/77 [==============================] - 14s 158ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2446it [00:00, 11429.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping downloads/evaluations/LSTUR_l0.01_d0.3/predictions.txt to downloads/evaluations/LSTUR_l0.01_d0.3/predictions.zip\n",
      "Evaluating combination: history_size=100, n_users=70000, title_size=50\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 400), dtype=tf.float32, name=None), name='att_layer2_8/Sum_1:0', description=\"created by layer 'att_layer2_8'\")\n",
      "37/37 [==============================] - ETA: 0s - loss: 2.9164\n",
      "Epoch 1: val_loss improved from inf to 0.00000, saving model to downloads/data/state_dict/LSTUR_l0.01_d0.5/weights\n",
      "37/37 [==============================] - 98s 3s/step - loss: 2.9164 - val_loss: 0.0000e+00\n",
      "77/77 [==============================] - 15s 156ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2446it [00:00, 11181.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping downloads/evaluations/LSTUR_l0.01_d0.5/predictions.txt to downloads/evaluations/LSTUR_l0.01_d0.5/predictions.zip\n",
      "All combinations evaluated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'history_size': [100],\n",
    "    'n_users':  [70000],\n",
    "    'title_size': [50],\n",
    "    'learning_rate': [0.0001, 0.001, 0.01],\n",
    "    'dropout':  [0.1, 0.3, 0.5]\n",
    "}\n",
    "\n",
    "combinations = list(\n",
    "    itertools.product(\n",
    "        param_grid['history_size'], param_grid['n_users'], param_grid['title_size'], param_grid['learning_rate'], param_grid['dropout']\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "all_results = []\n",
    "for history_size, n_users, title_size, learning_rate, dropout in combinations:\n",
    "    print(f\"Evaluating combination: history_size={history_size}, n_users={n_users}, title_size={title_size}\")\n",
    "    result = objective(history_size, n_users, title_size, learning_rate, dropout, df_validation, df_train)\n",
    "    all_results.append({\n",
    "        'history_size': history_size,\n",
    "        'n_users': n_users,\n",
    "        'title_size': title_size,\n",
    "        'evaluation_results': result\n",
    "    })\n",
    "\n",
    "# Save all results to a file\n",
    "with open(\"downloads/evaluations/all_results.txt\", 'w') as f:\n",
    "    for result in all_results:\n",
    "        f.write(f\"{result}\\n\")\n",
    "\n",
    "print(\"All combinations evaluated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
