{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTUR: Hyperparameter Optimization for the LSTUR Model\n",
    "\n",
    "This notebook focuses on optimizing the hyperparameters for the LSTUR model to achieve the better performance. For this approach we are using a search grid containing following search space:\n",
    "\n",
    "```python\n",
    "param_grid = {\n",
    "    'history_size': [10, 50, 100],\n",
    "    'n_users':  [20000, 50000, 70000],\n",
    "    'title_size': [10, 50, 100],\n",
    "    'learning_rate': [0.0001, 0.001, 0.01],\n",
    "    'dropout':  [0.1, 0.3, 0.5]\n",
    "}\n",
    "```\n",
    "\n",
    "LSTUR \\[2\\] is a news recommendation approach that captures users' both long-term preferences and short-term interests. The core of LSTUR is composed of a news encoder and a user encoder. The news encoder learns representations of news from their titles, while the user encoder learns long-term user representations from the embeddings of their IDs and short-term user representations from their recently browsed news via a GRU network.\n",
    "\n",
    "## Properties of LSTUR:\n",
    "\n",
    "- **Dual User Representations**: LSTUR captures both short-term and long-term preferences by using embeddings of users' IDs for long-term user representations and a GRU network to learn short-term user representations from recently browsed articles.\n",
    "- **News Encoder**: Utilizes the news titles to generate news representations.\n",
    "- **User Encoder**: Combines long-term and short-term user representations. Two methods are proposed for this combination:\n",
    "  - Initializing the hidden state of the GRU network with the long-term user representation.\n",
    "  - Concatenating both long-term and short-term user representations to form a unified user vector.\n",
    " \n",
    "\\[1\\] https://github.com/ebanalyse/ebnerd-benchmark\n",
    "\n",
    "\\[2\\] https://aclanthology.org/P19-1033/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "from ebrec.utils._constants import (\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_SUBTITLE_COL,\n",
    "    DEFAULT_TITLE_COL,\n",
    "    DEFAULT_USER_COL,\n",
    ")\n",
    "\n",
    "from ebrec.utils._behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    "    add_known_user_column,\n",
    "    add_prediction_scores,\n",
    "    truncate_history,\n",
    ")\n",
    "from ebrec.utils._articles import convert_text2encoding_with_transformers\n",
    "from ebrec.utils._polars import concat_str_columns, slice_join_dataframes\n",
    "from ebrec.utils._articles import create_article_id_to_value_mapping\n",
    "from ebrec.utils._nlp import get_transformers_word_embeddings\n",
    "from ebrec.utils._python import write_submission_file, rank_predictions_by_score\n",
    "from ebrec.evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "    \n",
    "from ebrec.models.newsrec.dataloader import LSTURDataLoader\n",
    "from ebrec.models.newsrec.model_config import hparams_lstur\n",
    "from ebrec.models.newsrec import LSTURModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Process Behavior and History Parquet Files\n",
    "The functions below are necessary for transforming the Ebnerd Dataset tables (`history.parquet` and `behaviors.parquet`) into a format suitable for training and testing. This transformation is achieved by joining the histories and behaviors based on the `user ID`. Additionally, preprocessing steps are performed, such as truncating the user history to keep only the specified `history_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ebnerd_from_path(path: Path, history_size: int = 30) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load ebnerd - function\n",
    "    \"\"\"\n",
    "    df_history = (\n",
    "        pl.scan_parquet(path.joinpath(\"history.parquet\"))\n",
    "        .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "        .pipe(\n",
    "            truncate_history,\n",
    "            column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "            history_size=history_size,\n",
    "            padding_value=0,\n",
    "            enable_warning=False,\n",
    "        )\n",
    "    )\n",
    "    df_behaviors = (\n",
    "        pl.scan_parquet(path.joinpath(\"behaviors.parquet\"))\n",
    "        .collect()\n",
    "        .pipe(\n",
    "            slice_join_dataframes,\n",
    "            df2=df_history.collect(),\n",
    "            on=DEFAULT_USER_COL,\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "    return df_behaviors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Path and Data Configuration\n",
    "Here we setup the `PATH` to the ebenrd dataset we are using for training. `COLUMNS` define the columns we are using for the model training, `TEXT_COLUMNS_TO_USE` contains columns should be considered in the embedding process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"LSTUR\"\n",
    "\n",
    "DATA_PATH = Path(\"~/shared/194.035-2024S/groups/Gruppe_33/Group_33/data\")\n",
    "DATASPLIT = \"small\"\n",
    "\n",
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "]\n",
    "\n",
    "HISTORY_SIZE = 50\n",
    "MAX_TITLE_LENGTH = 50\n",
    "\n",
    "TEXT_COLUMNS_TO_USE = [DEFAULT_SUBTITLE_COL, DEFAULT_TITLE_COL]\n",
    "FRACTION = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and Process Training and Validation Data\n",
    "This section uses the previously defined functions to create training and validation datasets. Additional preprocessing steps include applying the `sampling_strategy_wu2019` strategy, creating binary labels using `create_binary_labels_column`, and sampling a fraction of the data for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = (\n",
    "    ebnerd_from_path(DATA_PATH.joinpath(DATASPLIT, \"train\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=4,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=123,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "\n",
    "df_validation = (\n",
    "    ebnerd_from_path(DATA_PATH.joinpath(DATASPLIT, \"validation\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Articles\n",
    "In this cell we are loading the articles into memory which will later be used for the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles = pl.read_parquet(DATA_PATH.joinpath(DATASPLIT, \"articles.parquet\"))\n",
    "df_articles.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and Configure Transformer Model\n",
    "\n",
    "This cell loads a pre-trained transformer model and tokenizer from Hugging Face, specifically FacebookAI/xlm-roberta-base, establishing the NLP backbone for the notebook. The transformer model is critical for transforming raw text data into structured embeddings that can be effectively utilized within the recommendation system. The following steps are executed:\n",
    "- **Load Transformer Model and Tokenizer**: The AutoModel and AutoTokenizer from Hugging Face are used to load the pre-trained xlm-roberta-base model.\n",
    "- **Initialize Word Embeddings**: Word embeddings are initialized using the transformer's word embeddings to enhance the text representation.\n",
    "- **Concatenate Text Columns**: Text columns from the articles dataframe are concatenated to create a comprehensive text field.\n",
    "- **Convert Text to Encodings**: The concatenated text is tokenized and converted to numerical encodings using the transformer tokenizer, with a specified maximum length.\n",
    "- **Create Article Mapping**: A mapping from article IDs to their corresponding tokenized values is created, facilitating efficient lookup and processing in the recommendation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "TEXT_COLUMNS_TO_USE = [DEFAULT_SUBTITLE_COL, DEFAULT_TITLE_COL]\n",
    "\n",
    "# LOAD HUGGINGFACE:\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "# We'll init the word embeddings using the\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "#\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "    df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH\n",
    ")\n",
    "# =>\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=token_col_title\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading for Model Input and Model Configuration\n",
    "This cell creates data loaders for both training and validation. A mapping from user IDs to unique integer indices is created to facilitate embedding lookup. The `LSTURDataLoader` is initialized for both training and validation datasets, handling batching, shuffling, and input feature construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_mapping = {user_id: i for i, user_id in enumerate(df_train[DEFAULT_USER_COL].unique())}\n",
    "\n",
    "train_dataloader = LSTURDataLoader(\n",
    "    user_id_mapping=user_id_mapping,\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=64,\n",
    ")\n",
    "val_dataloader = LSTURDataLoader(\n",
    "    user_id_mapping=user_id_mapping,\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=True,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization for LSTUR Model\n",
    "In this section, we perform hyperparameter optimization to find the best combination of parameters for the LSTUR model. The optimization focuses on input dimensions such as `history_size`, `n_users`, and `title_size`, as well as model-specific parameters like `learning_rate` and `dropout`. The process involves evaluating different combinations of these parameters to identify the configuration that yields the best performance.\n",
    "\n",
    "### Setting Up Hyperparameter Optimization\n",
    "The `objective` function is defined to train the LSTUR model with a given set of hyperparameters and evaluate its performance. The function creates directories for logs and model weights, sets the hyperparameters, trains the model, and then evaluates it. The evaluation results are saved, and predictions are written to a submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(history_size, n_users, title_size, learning_rate, dropout, df_validation,df_train):\n",
    "\n",
    "    # Create directories for logs and model weights\n",
    "    MODEL_NAME = f\"LSTUR_l{learning_rate}_d{dropout}\"\n",
    "    LOG_DIR = f\"downloads/runs/{MODEL_NAME}\"\n",
    "    MODEL_WEIGHTS = f\"downloads/data/state_dict/{MODEL_NAME}/weights\"\n",
    "    RESULTS_DIR = f\"downloads/evaluations/{MODEL_NAME}\"\n",
    "    \n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=1)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2)\n",
    "    modelcheckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=MODEL_WEIGHTS, save_best_only=True, save_weights_only=True, verbose=1\n",
    "    )\n",
    "\n",
    "    # Set the parameters\n",
    "    hparams_lstur.history_size = history_size\n",
    "    hparams_lstur.n_users = n_users\n",
    "    hparams_lstur.title_size = title_size\n",
    "    hparams_lstur.learning_rate = learning_rate\n",
    "    hparams_lstur.dropout = dropout\n",
    "    \n",
    "\n",
    "    model = LSTURModel(\n",
    "        hparams=hparams_lstur,\n",
    "        word2vec_embedding=word2vec_embedding,\n",
    "        seed=42,\n",
    "    )\n",
    "    hist = model.model.fit(\n",
    "        train_dataloader,\n",
    "        validation_data=val_dataloader,\n",
    "        epochs=1,\n",
    "        callbacks=[tensorboard_callback, early_stopping, modelcheckpoint],\n",
    "    )\n",
    "    _ = model.model.load_weights(filepath=MODEL_WEIGHTS)\n",
    "\n",
    "\n",
    "    \n",
    "    pred_validation = model.scorer.predict(val_dataloader)\n",
    "    df_validation = add_prediction_scores(df_validation, pred_validation.tolist()).pipe(\n",
    "        add_known_user_column, known_users=df_train[DEFAULT_USER_COL]\n",
    "    )\n",
    "\n",
    "    metrics = MetricEvaluator(\n",
    "        labels=df_validation[\"labels\"].to_list(),\n",
    "        predictions=df_validation[\"scores\"].to_list(),\n",
    "        metric_functions=[AucScore(), MrrScore(), NdcgScore(k=5), NdcgScore(k=10)],\n",
    "    )\n",
    "    evaluation_results = metrics.evaluate().evaluations\n",
    "\n",
    "    # Save the evaluation results\n",
    "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "    with open(os.path.join(RESULTS_DIR, 'evaluation_results.txt'), 'w') as f:\n",
    "        for key, value in evaluation_results.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "    # Rank predictions and write submission file\n",
    "    df_validation = df_validation.with_columns(\n",
    "        pl.col(\"scores\")\n",
    "        .map_elements(lambda x: list(rank_predictions_by_score(x)))\n",
    "        .alias(\"ranked_scores\")\n",
    "    )\n",
    "    write_submission_file(\n",
    "        impression_ids=df_validation[DEFAULT_IMPRESSION_ID_COL],\n",
    "        prediction_scores=df_validation[\"ranked_scores\"],\n",
    "        path=os.path.join(RESULTS_DIR, \"predictions.txt\"),\n",
    "    )\n",
    "\n",
    "    return evaluation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Hyperparameter Optimization\n",
    "A grid of hyperparameter values is defined, and the `objective` function is called for each combination of these values. The results for each combination are stored and saved to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating combination: history_size=100, n_users=70000, title_size=50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 18:39:03.339396: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-19 18:39:03.345001: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-19 18:39:03.348078: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-19 18:39:03.352046: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-19 18:39:03.355090: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-19 18:39:03.358175: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-19 18:39:03.635084: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-19 18:39:03.637109: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-19 18:39:03.638990: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-19 18:39:03.640827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13775 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:06:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 400), dtype=tf.float32, name=None), name='att_layer2/Sum_1:0', description=\"created by layer 'att_layer2'\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 18:39:07.941617: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-06-19 18:39:15.400662: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-19 18:39:15.670208: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-06-19 18:39:18.300474: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f7bd5fac8f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-19 18:39:18.300548: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2024-06-19 18:39:18.316275: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1718822358.523325   11580 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - ETA: 0s - loss: 1.5978"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 18:44:06.538086: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 46080368640 exceeds 10% of free system memory.\n",
      "2024-06-19 18:44:35.844144: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 46080368640 exceeds 10% of free system memory.\n",
      "2024-06-19 18:45:02.633908: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 6720096000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.00000, saving model to downloads/data/state_dict/LSTUR_l0.0001_d0.1/weights\n",
      "37/37 [==============================] - 363s 10s/step - loss: 1.5978 - val_loss: 0.0000e+00\n",
      "77/77 [==============================] - 14s 161ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2446it [00:00, 10941.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping downloads/evaluations/LSTUR_l0.0001_d0.1/predictions.txt to downloads/evaluations/LSTUR_l0.0001_d0.1/predictions.zip\n",
      "Evaluating combination: history_size=100, n_users=70000, title_size=50\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 400), dtype=tf.float32, name=None), name='att_layer2_1/Sum_1:0', description=\"created by layer 'att_layer2_1'\")\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.5979"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 18:46:10.737159: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 46080368640 exceeds 10% of free system memory.\n",
      "2024-06-19 18:46:40.059043: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 46080368640 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.00000, saving model to downloads/data/state_dict/LSTUR_l0.0001_d0.3/weights\n",
      "37/37 [==============================] - 98s 3s/step - loss: 1.5979 - val_loss: 0.0000e+00\n",
      "77/77 [==============================] - 14s 160ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2446it [00:00, 11404.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping downloads/evaluations/LSTUR_l0.0001_d0.3/predictions.txt to downloads/evaluations/LSTUR_l0.0001_d0.3/predictions.zip\n",
      "Evaluating combination: history_size=100, n_users=70000, title_size=50\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 400), dtype=tf.float32, name=None), name='att_layer2_2/Sum_1:0', description=\"created by layer 'att_layer2_2'\")\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.6087\n",
      "Epoch 1: val_loss improved from inf to 0.00000, saving model to downloads/data/state_dict/LSTUR_l0.0001_d0.5/weights\n",
      "37/37 [==============================] - 97s 3s/step - loss: 1.6087 - val_loss: 0.0000e+00\n",
      "77/77 [==============================] - 14s 165ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2446it [00:00, 11361.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping downloads/evaluations/LSTUR_l0.0001_d0.5/predictions.txt to downloads/evaluations/LSTUR_l0.0001_d0.5/predictions.zip\n",
      "Evaluating combination: history_size=100, n_users=70000, title_size=50\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 400), dtype=tf.float32, name=None), name='att_layer2_3/Sum_1:0', description=\"created by layer 'att_layer2_3'\")\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.5879\n",
      "Epoch 1: val_loss improved from inf to 0.00000, saving model to downloads/data/state_dict/LSTUR_l0.001_d0.1/weights\n",
      "37/37 [==============================] - 100s 3s/step - loss: 1.5879 - val_loss: 0.0000e+00\n",
      "77/77 [==============================] - 14s 159ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2446it [00:00, 11382.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping downloads/evaluations/LSTUR_l0.001_d0.1/predictions.txt to downloads/evaluations/LSTUR_l0.001_d0.1/predictions.zip\n",
      "Evaluating combination: history_size=100, n_users=70000, title_size=50\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 400), dtype=tf.float32, name=None), name='att_layer2_4/Sum_1:0', description=\"created by layer 'att_layer2_4'\")\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.6053\n",
      "Epoch 1: val_loss improved from inf to 0.00000, saving model to downloads/data/state_dict/LSTUR_l0.001_d0.3/weights\n",
      "37/37 [==============================] - 98s 2s/step - loss: 1.6053 - val_loss: 0.0000e+00\n",
      "77/77 [==============================] - 14s 159ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2446it [00:00, 11402.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping downloads/evaluations/LSTUR_l0.001_d0.3/predictions.txt to downloads/evaluations/LSTUR_l0.001_d0.3/predictions.zip\n",
      "Evaluating combination: history_size=100, n_users=70000, title_size=50\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 400), dtype=tf.float32, name=None), name='att_layer2_5/Sum_1:0', description=\"created by layer 'att_layer2_5'\")\n",
      "37/37 [==============================] - ETA: 0s - loss: 1.6184\n",
      "Epoch 1: val_loss improved from inf to 0.00000, saving model to downloads/data/state_dict/LSTUR_l0.001_d0.5/weights\n",
      "37/37 [==============================] - 99s 3s/step - loss: 1.6184 - val_loss: 0.0000e+00\n",
      "77/77 [==============================] - 14s 158ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2446it [00:00, 11446.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping downloads/evaluations/LSTUR_l0.001_d0.5/predictions.txt to downloads/evaluations/LSTUR_l0.001_d0.5/predictions.zip\n",
      "Evaluating combination: history_size=100, n_users=70000, title_size=50\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 400), dtype=tf.float32, name=None), name='att_layer2_6/Sum_1:0', description=\"created by layer 'att_layer2_6'\")\n",
      "37/37 [==============================] - ETA: 0s - loss: 4.1866\n",
      "Epoch 1: val_loss improved from inf to 0.00000, saving model to downloads/data/state_dict/LSTUR_l0.01_d0.1/weights\n",
      "37/37 [==============================] - 98s 3s/step - loss: 4.1866 - val_loss: 0.0000e+00\n",
      "77/77 [==============================] - 14s 159ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2446it [00:00, 11311.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping downloads/evaluations/LSTUR_l0.01_d0.1/predictions.txt to downloads/evaluations/LSTUR_l0.01_d0.1/predictions.zip\n",
      "Evaluating combination: history_size=100, n_users=70000, title_size=50\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 400), dtype=tf.float32, name=None), name='att_layer2_7/Sum_1:0', description=\"created by layer 'att_layer2_7'\")\n",
      "37/37 [==============================] - ETA: 0s - loss: 3.4180\n",
      "Epoch 1: val_loss improved from inf to 0.00000, saving model to downloads/data/state_dict/LSTUR_l0.01_d0.3/weights\n",
      "37/37 [==============================] - 98s 3s/step - loss: 3.4180 - val_loss: 0.0000e+00\n",
      "77/77 [==============================] - 14s 158ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2446it [00:00, 11429.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping downloads/evaluations/LSTUR_l0.01_d0.3/predictions.txt to downloads/evaluations/LSTUR_l0.01_d0.3/predictions.zip\n",
      "Evaluating combination: history_size=100, n_users=70000, title_size=50\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 400), dtype=tf.float32, name=None), name='att_layer2_8/Sum_1:0', description=\"created by layer 'att_layer2_8'\")\n",
      "37/37 [==============================] - ETA: 0s - loss: 2.9164\n",
      "Epoch 1: val_loss improved from inf to 0.00000, saving model to downloads/data/state_dict/LSTUR_l0.01_d0.5/weights\n",
      "37/37 [==============================] - 98s 3s/step - loss: 2.9164 - val_loss: 0.0000e+00\n",
      "77/77 [==============================] - 15s 156ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2446it [00:00, 11181.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping downloads/evaluations/LSTUR_l0.01_d0.5/predictions.txt to downloads/evaluations/LSTUR_l0.01_d0.5/predictions.zip\n",
      "All combinations evaluated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'history_size': [100],\n",
    "    'n_users':  [70000],\n",
    "    'title_size': [50],\n",
    "    'learning_rate': [0.0001, 0.001, 0.01],\n",
    "    'dropout':  [0.1, 0.3, 0.5]\n",
    "}\n",
    "\n",
    "combinations = list(\n",
    "    itertools.product(\n",
    "        param_grid['history_size'], param_grid['n_users'], param_grid['title_size'], param_grid['learning_rate'], param_grid['dropout']\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "all_results = []\n",
    "for history_size, n_users, title_size, learning_rate, dropout in combinations:\n",
    "    print(f\"Evaluating combination: history_size={history_size}, n_users={n_users}, title_size={title_size}\")\n",
    "    result = objective(history_size, n_users, title_size, learning_rate, dropout, df_validation, df_train)\n",
    "    all_results.append({\n",
    "        'history_size': history_size,\n",
    "        'n_users': n_users,\n",
    "        'title_size': title_size,\n",
    "        'evaluation_results': result\n",
    "    })\n",
    "\n",
    "# Save all results to a file\n",
    "with open(\"downloads/evaluations/all_results.txt\", 'w') as f:\n",
    "    for result in all_results:\n",
    "        f.write(f\"{result}\\n\")\n",
    "\n",
    "print(\"All combinations evaluated.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
