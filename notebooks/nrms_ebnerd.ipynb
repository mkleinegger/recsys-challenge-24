{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Content-Based Filtering\n",
    "This notebook describes the baseline content-based filtering model for news recommendation using NRMS (Neural News Recommendation with Multi-head Self-attention)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NRMS (Neural News Recommendation with Multi-Head Self-Attention):  \n",
    " The NRMS [1] model is a state-of-the-art neural news recommendation approach with multi-head self-attention. NRMS learns news representations from news titles, using a multi head self-attention network. Briefly speaking, the news encoder first maps each word in the news title to the corresponding vector, and then uses the self-attention network to learn word-level representations. Finally, a query vector is used to locate the important words in the news title, and an attention-based pooling method is used to aggregate the word-level representations into the learned title representations. To learn user representations from their browsed news, NRMS again uses the multi-head self-attention network on top of the learned news representations. The probability of a user clicking a candidate news is given by the dot product between the user representations and the news representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components:\n",
    "\n",
    "### News Encoder:\n",
    "- Uses multi-head self-attention to learn news representations from news titles\n",
    "- Models interactions between words\n",
    "  \n",
    "### User Encoder:\n",
    "\n",
    "- Learns user representations from their browsed news\n",
    "  \n",
    "## Key Features:\n",
    "- It helps users find the news they like\n",
    "- Avoids information overload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T16:58:44.274226Z",
     "start_time": "2024-05-18T16:58:39.936186Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%cd ebnerd-benchmark\n",
    "#!pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the NRMS Model from News Recommendation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T16:58:47.920171Z",
     "start_time": "2024-05-18T16:58:47.839314Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 13:36:07.131173: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-05 13:36:07.694989: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-05 13:36:07.695063: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-05 13:36:07.711922: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-05 13:36:07.758303: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-05 13:36:12.491430: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "\n",
    "# Import Constants and Utilities\n",
    "from ebrec.utils._constants import (\n",
    "   DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "   DEFAULT_CLICKED_ARTICLES_COL,\n",
    "   DEFAULT_INVIEW_ARTICLES_COL,\n",
    "   DEFAULT_IMPRESSION_ID_COL,\n",
    "   DEFAULT_SUBTITLE_COL,\n",
    "   DEFAULT_LABELS_COL,\n",
    "   DEFAULT_TITLE_COL,\n",
    "   DEFAULT_USER_COL,\n",
    ")\n",
    "\n",
    "#Import Utility Functions\n",
    "#\n",
    "from ebrec.utils._behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    "    add_known_user_column,\n",
    "    add_prediction_scores,\n",
    "    truncate_history,\n",
    ")\n",
    "from ebrec.utils._articles import convert_text2encoding_with_transformers\n",
    "from ebrec.utils._polars import concat_str_columns, slice_join_dataframes\n",
    "from ebrec.utils._articles import create_article_id_to_value_mapping\n",
    "from ebrec.utils._nlp import get_transformers_word_embeddings\n",
    "\n",
    "#Import NRMS Components\n",
    "#\n",
    "from ebrec.models.newsrec.dataloader import NRMSDataLoader\n",
    "from ebrec.models.newsrec.model_config import hparams_nrms\n",
    "from ebrec.models.newsrec import NRMSModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NRMSDataLoader loads and preprocesses data for the NRMS model.\n",
    "\n",
    "hparams_nrms provides configuration parameters for the NRMS model.\n",
    "\n",
    "NRMSModel defines the architecture and training procedures for NRMS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up News Recommendation Model Components \n",
    "In this step, we configure essential components for the news recommendation model, including data preprocessing utilities and the NRMS model setup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Required Modules and Componentsabs\n",
    "\n",
    "from ebrec.utils._articles import convert_text2encoding_with_transformers\n",
    "from ebrec.utils._polars import concat_str_columns, slice_join_dataframes\n",
    "from ebrec.utils._articles import create_article_id_to_value_mapping\n",
    "from ebrec.utils._nlp import get_transformers_word_embeddings\n",
    "\n",
    "#\n",
    "from ebrec.models.newsrec.dataloader import NRMSDataLoader\n",
    "from ebrec.models.newsrec.model_config import hparams_nrms\n",
    "from ebrec.models.newsrec import NRMSModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrated Behavioral Data\n",
    "This step integrates historical article interactions with behavioral data, facilitating data coherence and readiness for analysis\n",
    "\n",
    "## Data Format Description\n",
    "1. history.parquet\n",
    "File Content: Contains historical interaction data between users and articles.\n",
    "Columns Used:\n",
    "DEFAULT_USER_COL: Represents the user identifier.\n",
    "DEFAULT_HISTORY_ARTICLE_ID_COL: Represents the article identifier that the user interacted with.\n",
    "\n",
    "2. behaviors.parquet\n",
    "File Content: Contains behavioral data related to user interactions.\n",
    "Columns:\n",
    "Various columns related to user behavior and possibly article metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ebnerd_from_path(path: Path, history_size: int = 30) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Load ebnerd - function\n",
    "    \"\"\"\n",
    "    df_history = (\n",
    "        pl.scan_parquet(path.joinpath(\"history.parquet\"))\n",
    "        .select(DEFAULT_USER_COL, DEFAULT_HISTORY_ARTICLE_ID_COL)\n",
    "        .pipe(\n",
    "            truncate_history,\n",
    "            column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "            history_size=history_size,\n",
    "            padding_value=0,\n",
    "            enable_warning=False\n",
    "        )\n",
    "    )\n",
    "    df_behaviors = (\n",
    "        pl.scan_parquet(path.joinpath(\"behaviors.parquet\"))\n",
    "        .collect()\n",
    "        .pipe(\n",
    "            slice_join_dataframes,\n",
    "            df2=df_history.collect(),\n",
    "            on=DEFAULT_USER_COL,\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "    return df_behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path(\"/home/e12242664/shared/194.035-2024S/groups/Gruppe_33/Group_33/downloads\")\n",
    "DATASPLIT = \"demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Data Preparation for Training\n",
    "Loading and preprocessing data from the training split of the EB-NeRD dataset\n",
    "\n",
    "### COLUMNS: \n",
    "Defines a list of column names that are of interest when loading and processing data from the ebnerd dataset.\n",
    "### HISTORY_SIZE:\n",
    "Sets the maximum number of historical articles per user to retain (used in ebnerd_from_path function).\n",
    "### FRACTION:\n",
    "Determines the fraction of data to sample for training and validation.\n",
    "\n",
    "### ebnerd_from_path: \n",
    "Loads and processes training data (train split) using historical interaction data and behavioral features.\n",
    "### select(COLUMNS): \n",
    "Selects specific columns defined in COLUMNS.\n",
    "sampling_strategy_wu2019: Applies sampling strategy based on the Wu et al. (2019) methodology.\n",
    "### Methodology: \n",
    "This algorithm adjusts the dataset to maintain a balanced ratio (npratio=4) between negative and positive samples (typically in recommendation systems).\n",
    "### create_binary_labels_column:\n",
    "Creates binary labels (e.g., 0 or 1) based on relevance.\n",
    "### sample(fraction=FRACTION):\n",
    "Samples a fraction of the dataset for efficiency or testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i64]</td><td>list[i64]</td><td>u32</td><td>list[i8]</td></tr></thead><tbody><tr><td>22779</td><td>[9767624, 9767675, … 9770541]</td><td>[9774461, 9759966, … 9759544]</td><td>[9759966]</td><td>48401</td><td>[0, 1, … 0]</td></tr><tr><td>150224</td><td>[9755821, 9759109, … 9735909]</td><td>[9778682, 9777397, … 9482970]</td><td>[9778661]</td><td>152513</td><td>[0, 0, … 0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 6)\n",
       "┌─────────┬───────────────────┬───────────────────┬──────────────────┬───────────────┬─────────────┐\n",
       "│ user_id ┆ article_id_fixed  ┆ article_ids_invie ┆ article_ids_clic ┆ impression_id ┆ labels      │\n",
       "│ ---     ┆ ---               ┆ w                 ┆ ked              ┆ ---           ┆ ---         │\n",
       "│ u32     ┆ list[i32]         ┆ ---               ┆ ---              ┆ u32           ┆ list[i8]    │\n",
       "│         ┆                   ┆ list[i64]         ┆ list[i64]        ┆               ┆             │\n",
       "╞═════════╪═══════════════════╪═══════════════════╪══════════════════╪═══════════════╪═════════════╡\n",
       "│ 22779   ┆ [9767624,         ┆ [9774461,         ┆ [9759966]        ┆ 48401         ┆ [0, 1, … 0] │\n",
       "│         ┆ 9767675, …        ┆ 9759966, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9770541]          ┆ 9759544]          ┆                  ┆               ┆             │\n",
       "│ 150224  ┆ [9755821,         ┆ [9778682,         ┆ [9778661]        ┆ 152513        ┆ [0, 0, … 0] │\n",
       "│         ┆ 9759109, …        ┆ 9777397, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9735909]          ┆ 9482970]          ┆                  ┆               ┆             │\n",
       "└─────────┴───────────────────┴───────────────────┴──────────────────┴───────────────┴─────────────┘"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "]\n",
    "HISTORY_SIZE = 50\n",
    "FRACTION = 1\n",
    "\n",
    "df_train = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"train\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=4,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=123,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "# =>\n",
    "df_validation = (\n",
    "    ebnerd_from_path(PATH.joinpath(DATASPLIT, \"validation\"), history_size=HISTORY_SIZE)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "\n",
    "df_test = (ebnerd_from_path(PATH.joinpath(\"test\"), history_size=HISTORY_SIZE)\n",
    "    .select([\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "])\n",
    "  \n",
    "    .sample(fraction=FRACTION)\n",
    "          )\n",
    "    \n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the difference between Training/Validation and Testset\n",
    "Note, the testset doesn't include labels, and we have remove some of the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i64]</td><td>list[i64]</td><td>u32</td><td>list[i8]</td></tr></thead><tbody><tr><td>22779</td><td>[9767624, 9767675, … 9770541]</td><td>[9774461, 9759966, … 9759544]</td><td>[9759966]</td><td>48401</td><td>[0, 1, … 0]</td></tr><tr><td>150224</td><td>[9755821, 9759109, … 9735909]</td><td>[9778682, 9777397, … 9482970]</td><td>[9778661]</td><td>152513</td><td>[0, 0, … 0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 6)\n",
       "┌─────────┬───────────────────┬───────────────────┬──────────────────┬───────────────┬─────────────┐\n",
       "│ user_id ┆ article_id_fixed  ┆ article_ids_invie ┆ article_ids_clic ┆ impression_id ┆ labels      │\n",
       "│ ---     ┆ ---               ┆ w                 ┆ ked              ┆ ---           ┆ ---         │\n",
       "│ u32     ┆ list[i32]         ┆ ---               ┆ ---              ┆ u32           ┆ list[i8]    │\n",
       "│         ┆                   ┆ list[i64]         ┆ list[i64]        ┆               ┆             │\n",
       "╞═════════╪═══════════════════╪═══════════════════╪══════════════════╪═══════════════╪═════════════╡\n",
       "│ 22779   ┆ [9767624,         ┆ [9774461,         ┆ [9759966]        ┆ 48401         ┆ [0, 1, … 0] │\n",
       "│         ┆ 9767675, …        ┆ 9759966, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9770541]          ┆ 9759544]          ┆                  ┆               ┆             │\n",
       "│ 150224  ┆ [9755821,         ┆ [9778682,         ┆ [9778661]        ┆ 152513        ┆ [0, 0, … 0] │\n",
       "│         ┆ 9759109, …        ┆ 9777397, …        ┆                  ┆               ┆             │\n",
       "│         ┆ 9735909]          ┆ 9482970]          ┆                  ┆               ┆             │\n",
       "└─────────┴───────────────────┴───────────────────┴──────────────────┴───────────────┴─────────────┘"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>impression_id</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i32]</td><td>u32</td></tr></thead><tbody><tr><td>35982</td><td>[9782499, 9783024, … 9789494]</td><td>[9796527, 7851321, … 9492777]</td><td>6451339</td></tr><tr><td>36012</td><td>[9786247, 9786209, … 9790885]</td><td>[9798532, 9791602, … 9798958]</td><td>6451363</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 4)\n",
       "┌─────────┬───────────────────────────────┬───────────────────────────────┬───────────────┐\n",
       "│ user_id ┆ article_id_fixed              ┆ article_ids_inview            ┆ impression_id │\n",
       "│ ---     ┆ ---                           ┆ ---                           ┆ ---           │\n",
       "│ u32     ┆ list[i32]                     ┆ list[i32]                     ┆ u32           │\n",
       "╞═════════╪═══════════════════════════════╪═══════════════════════════════╪═══════════════╡\n",
       "│ 35982   ┆ [9782499, 9783024, … 9789494] ┆ [9796527, 7851321, … 9492777] ┆ 6451339       │\n",
       "│ 36012   ┆ [9786247, 9786209, … 9790885] ┆ [9798532, 9791602, … 9798958] ┆ 6451363       │\n",
       "└─────────┴───────────────────────────────┴───────────────────────────────┴───────────────┘"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>article_id</th><th>title</th><th>subtitle</th><th>last_modified_time</th><th>premium</th><th>body</th><th>published_time</th><th>image_ids</th><th>article_type</th><th>url</th><th>ner_clusters</th><th>entity_groups</th><th>topics</th><th>category</th><th>subcategory</th><th>category_str</th><th>total_inviews</th><th>total_pageviews</th><th>total_read_time</th><th>sentiment_score</th><th>sentiment_label</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>datetime[μs]</td><td>bool</td><td>str</td><td>datetime[μs]</td><td>list[i64]</td><td>str</td><td>str</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>i16</td><td>list[i16]</td><td>str</td><td>i32</td><td>i32</td><td>f32</td><td>f32</td><td>str</td></tr></thead><tbody><tr><td>3037230</td><td>&quot;Ishockey-spill…</td><td>&quot;ISHOCKEY: Isho…</td><td>2023-06-29 06:20:57</td><td>false</td><td>&quot;Ambitionerne o…</td><td>2003-08-28 08:55:00</td><td>null</td><td>&quot;article_defaul…</td><td>&quot;https://ekstra…</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Kendt&quot;, … &quot;Mindre ulykke&quot;]</td><td>142</td><td>[327, 334]</td><td>&quot;sport&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9752</td><td>&quot;Negative&quot;</td></tr><tr><td>3044020</td><td>&quot;Prins Harry tv…</td><td>&quot;Hoffet tvang P…</td><td>2023-06-29 06:21:16</td><td>false</td><td>&quot;Den britiske t…</td><td>2005-06-29 08:47:00</td><td>[3097307, 3097197, 3104927]</td><td>&quot;article_defaul…</td><td>&quot;https://ekstra…</td><td>[&quot;Harry&quot;, &quot;James Hewitt&quot;]</td><td>[&quot;PER&quot;, &quot;PER&quot;]</td><td>[&quot;Kriminalitet&quot;, &quot;Kendt&quot;, … &quot;Personfarlig kriminalitet&quot;]</td><td>414</td><td>[432]</td><td>&quot;underholdning&quot;</td><td>null</td><td>null</td><td>null</td><td>0.7084</td><td>&quot;Negative&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 21)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ article_i ┆ title     ┆ subtitle  ┆ last_modi ┆ … ┆ total_pag ┆ total_rea ┆ sentiment ┆ sentimen │\n",
       "│ d         ┆ ---       ┆ ---       ┆ fied_time ┆   ┆ eviews    ┆ d_time    ┆ _score    ┆ t_label  │\n",
       "│ ---       ┆ str       ┆ str       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ i32       ┆           ┆           ┆ datetime[ ┆   ┆ i32       ┆ f32       ┆ f32       ┆ str      │\n",
       "│           ┆           ┆           ┆ μs]       ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 3037230   ┆ Ishockey- ┆ ISHOCKEY: ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9752    ┆ Negative │\n",
       "│           ┆ spiller:  ┆ Ishockey- ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ Jeg       ┆ spilleren ┆ 06:20:57  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ troede    ┆ Seb…      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ jeg…      ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3044020   ┆ Prins     ┆ Hoffet    ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.7084    ┆ Negative │\n",
       "│           ┆ Harry     ┆ tvang     ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ tvunget   ┆ Prins     ┆ 06:21:16  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ til       ┆ Harry til ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ dna-test  ┆ at …      ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles = pl.read_parquet(PATH.joinpath(DATASPLIT, \"articles.parquet\"))\n",
    "df_articles.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Article Text with BERT\n",
    "This step involves initializing and loading a pre-trained BERT model (bert-base-multilingual-cased) from Hugging Face.\n",
    "\n",
    "### Transformer Model: \n",
    "Initializes a pre-trained BERT model (bert-base-multilingual-cased) from Hugging Face's Transformers library.\n",
    "### Tokenizer:\n",
    "Initializes a corresponding tokenizer for the BERT model to preprocess text inputs.\n",
    "## Constants:\n",
    "### TEXT_COLUMNS_TO_USE: \n",
    "Specifies which columns (DEFAULT_SUBTITLE_COL and DEFAULT_TITLE_COL) from df_articles to use for text processing.\n",
    "### MAX_TITLE_LENGTH: \n",
    "Defines the maximum length for tokenizing the title text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e12242664/recsys/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "TRANSFORMER_MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "TEXT_COLUMNS_TO_USE = [DEFAULT_SUBTITLE_COL, DEFAULT_TITLE_COL]\n",
    "MAX_TITLE_LENGTH = 30\n",
    "\n",
    "# LOAD HUGGINGFACE:\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "# We'll init the word embeddings using the\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "#\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "    df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH\n",
    ")\n",
    "# =>\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=token_col_title\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate the dataloaders\n",
    "In the implementations we have disconnected the models and data. Hence, you should built a dataloader that fits your needs.\n",
    "\n",
    "### behaviors:\n",
    "Provides the training dataset (df_train), which likely includes user behavior and interaction data.\n",
    "### article_dict: \n",
    "Dictionary mapping article IDs to their respective token embeddings (article_mapping), facilitating efficient lookup during training.\n",
    "### unknown_representation: \n",
    "Specifies how unknown articles are represented (e.g., as zero vectors).\n",
    "### history_column:\n",
    "Column in behaviors representing the user's historical interactions (DEFAULT_HISTORY_ARTICLE_ID_COL).\n",
    "### eval_mode=False:\n",
    "Indicates training mode, where the data loader prepares batches for model training.\n",
    "### batch_size=64:\n",
    "Determines the number of samples (user interactions) in each batch during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=64,\n",
    ")\n",
    "val_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=True,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "### train_dataloader: \n",
    "DataLoader for training data.\n",
    "### validation_data:\n",
    "DataLoader for validation data.\n",
    "### epochs=10:\n",
    "Number of training epochs.\n",
    "### callbacks: \n",
    "List of callbacks to monitor training and save model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "389/389 [==============================] - ETA: 0s - loss: 1.5238\n",
      "Epoch 1: val_loss improved from inf to 0.00000, saving model to /home/e12242664/shared/194.035-2024S/groups/Gruppe_33/Group_33/downloads/data/state_dict/NRMS/weights\n",
      "389/389 [==============================] - 697s 2s/step - loss: 1.5238 - val_loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "389/389 [==============================] - ETA: 0s - loss: 1.4259\n",
      "Epoch 2: val_loss did not improve from 0.00000\n",
      "389/389 [==============================] - 695s 2s/step - loss: 1.4259 - val_loss: 0.0000e+00\n",
      "Epoch 3/10\n",
      "389/389 [==============================] - ETA: 0s - loss: 1.4052\n",
      "Epoch 3: val_loss did not improve from 0.00000\n",
      "389/389 [==============================] - 704s 2s/step - loss: 1.4052 - val_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"NRMS\"\n",
    "LOG_DIR = PATH.joinpath(f\"data/{MODEL_NAME}\")\n",
    "MODEL_WEIGHTS = PATH.joinpath(f\"data/state_dict/{MODEL_NAME}/weights\")\n",
    "\n",
    "# CALLBACKS\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=1)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2)\n",
    "modelcheckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=MODEL_WEIGHTS, save_best_only=True, save_weights_only=True, verbose=1\n",
    ")\n",
    "\n",
    "hparams_nrms.history_size = HISTORY_SIZE\n",
    "model = NRMSModel(\n",
    "    hparams=hparams_nrms,\n",
    "    word2vec_embedding=word2vec_embedding,\n",
    "    seed=42,\n",
    ")\n",
    "hist = model.model.fit(\n",
    "    train_dataloader,\n",
    "    validation_data=val_dataloader,\n",
    "    epochs=10,\n",
    "    callbacks=[tensorboard_callback, early_stopping, modelcheckpoint],\n",
    ")\n",
    "_ = model.model.load_weights(filepath=MODEL_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example how to compute some metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "793/793 [==============================] - 528s 666ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_validation = model.scorer.predict(val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>article_id_fixed</th><th>article_ids_inview</th><th>article_ids_clicked</th><th>impression_id</th><th>labels</th><th>scores</th><th>is_known_user</th></tr><tr><td>u32</td><td>list[i32]</td><td>list[i32]</td><td>list[i32]</td><td>u32</td><td>list[i8]</td><td>list[f64]</td><td>bool</td></tr></thead><tbody><tr><td>76658</td><td>[9766238, 9767642, … 9779045]</td><td>[9787499, 9783042, … 9780702]</td><td>[9783042]</td><td>144772</td><td>[0, 1, … 0]</td><td>[0.383537, 0.401613, … 0.379169]</td><td>true</td></tr><tr><td>76658</td><td>[9766238, 9767642, … 9779045]</td><td>[9788352, 6741781, … 9788125]</td><td>[9788125]</td><td>144777</td><td>[0, 0, … 1]</td><td>[0.470372, 0.156016, … 0.490298]</td><td>true</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 8)\n",
       "┌─────────┬────────────┬────────────┬────────────┬────────────┬────────────┬───────────┬───────────┐\n",
       "│ user_id ┆ article_id ┆ article_id ┆ article_id ┆ impression ┆ labels     ┆ scores    ┆ is_known_ │\n",
       "│ ---     ┆ _fixed     ┆ s_inview   ┆ s_clicked  ┆ _id        ┆ ---        ┆ ---       ┆ user      │\n",
       "│ u32     ┆ ---        ┆ ---        ┆ ---        ┆ ---        ┆ list[i8]   ┆ list[f64] ┆ ---       │\n",
       "│         ┆ list[i32]  ┆ list[i32]  ┆ list[i32]  ┆ u32        ┆            ┆           ┆ bool      │\n",
       "╞═════════╪════════════╪════════════╪════════════╪════════════╪════════════╪═══════════╪═══════════╡\n",
       "│ 76658   ┆ [9766238,  ┆ [9787499,  ┆ [9783042]  ┆ 144772     ┆ [0, 1, …   ┆ [0.383537 ┆ true      │\n",
       "│         ┆ 9767642, … ┆ 9783042, … ┆            ┆            ┆ 0]         ┆ ,         ┆           │\n",
       "│         ┆ 9779045]   ┆ 9780702]   ┆            ┆            ┆            ┆ 0.401613, ┆           │\n",
       "│         ┆            ┆            ┆            ┆            ┆            ┆ …         ┆           │\n",
       "│         ┆            ┆            ┆            ┆            ┆            ┆ 0.379169] ┆           │\n",
       "│ 76658   ┆ [9766238,  ┆ [9788352,  ┆ [9788125]  ┆ 144777     ┆ [0, 0, …   ┆ [0.470372 ┆ true      │\n",
       "│         ┆ 9767642, … ┆ 6741781, … ┆            ┆            ┆ 1]         ┆ ,         ┆           │\n",
       "│         ┆ 9779045]   ┆ 9788125]   ┆            ┆            ┆            ┆ 0.156016, ┆           │\n",
       "│         ┆            ┆            ┆            ┆            ┆            ┆ …         ┆           │\n",
       "│         ┆            ┆            ┆            ┆            ┆            ┆ 0.490298] ┆           │\n",
       "└─────────┴────────────┴────────────┴────────────┴────────────┴────────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation = add_prediction_scores(df_validation, pred_validation.tolist()).pipe(\n",
    "    add_known_user_column, known_users=df_train[DEFAULT_USER_COL]\n",
    ")\n",
    "df_validation.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MetricEvaluator class>: \n",
       " {\n",
       "    \"auc\": 0.5456715539995841,\n",
       "    \"mrr\": 0.3341141497438294,\n",
       "    \"ndcg@5\": 0.37227371388113534,\n",
       "    \"ndcg@10\": 0.4530279432053414\n",
       "}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ebrec.evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "\n",
    "metrics = MetricEvaluator(\n",
    "    labels=df_validation[\"labels\"].to_list(),\n",
    "    predictions=df_validation[\"scores\"].to_list(),\n",
    "    metric_functions=[AucScore(), MrrScore(), NdcgScore(k=5), NdcgScore(k=10)],\n",
    ")\n",
    "metrics.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "[1] Yichao Lu, \"Bag of Tricks and a Strong Baseline for Neural News Recommendation,\" Layer 6 AI, https://msnews.github.io/assets/doc/3.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "recsys"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
